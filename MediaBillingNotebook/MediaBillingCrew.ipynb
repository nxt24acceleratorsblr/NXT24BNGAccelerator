{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef392679-3088-45dd-b075-93fc41422e81",
   "metadata": {},
   "source": [
    "# MEDIA BILLING RECONCILIATION WITH AGENTIC AIâ€‹\n",
    "\n",
    "---\n",
    "\n",
    "## DESCRIPTION\n",
    "\n",
    "This notebook implements a **File Reader Agent** that can ingest media invoices and billing data in multiple formats and extract structured information for processing.\n",
    "\n",
    "The agent uses CrewAI to intelligently read and parse files, making it easy to integrate with billing reconciliation workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ CAPABILITIES\n",
    "\n",
    "**File Format Support:**\n",
    "- ðŸ“„ **PDF** - Extract text from invoice PDFs\n",
    "- ðŸ“Š **CSV** - Parse tabular billing data\n",
    "- ðŸ“ˆ **Excel** (.xlsx, .xls) - Read spreadsheet data\n",
    "- ðŸ–¼ï¸ **Images** - OCR text extraction from scanned documents\n",
    "- ðŸ“ **Text** - Plain text file reading\n",
    "- ðŸ”– **XML** - Parse XML structured data\n",
    "\n",
    "**Agent Features:**\n",
    "- Single unified agent for all file types\n",
    "- Automatic file type detection\n",
    "- Structured data extraction\n",
    "- Error handling and validation\n",
    "- Ready for CrewAI integration\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– WHAT WE'RE BUILDING\n",
    "\n",
    "A **File Reader Agent** that:\n",
    "1. Accepts file paths as input\n",
    "2. Automatically detects file format\n",
    "3. Extracts and structures the data\n",
    "4. Returns clean, parsed content\n",
    "5. Integrates with CrewAI workflows\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ USE CASE\n",
    "\n",
    "This agent serves as the **data ingestion layer** for the media billing reconciliation system:\n",
    "- **Input**: Invoice files (PDF, CSV, Excel, images, XML)\n",
    "- **Process**: Read, parse, and structure data\n",
    "- **Output**: Standardized data for reconciliation agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e36c5",
   "metadata": {},
   "source": [
    "## âœ¨ Enhanced Excel Reading Capabilities\n",
    "\n",
    "The Excel file reader has been significantly improved with the following features:\n",
    "\n",
    "### ðŸŽ¯ What's New:\n",
    "\n",
    "1. **Multi-Sheet Support**\n",
    "   - Automatically detects all sheets in Excel files\n",
    "   - Reads all sheets or specific sheets by name/index\n",
    "   - Provides sheet-by-sheet analysis\n",
    "\n",
    "2. **Intelligent Data Analysis**\n",
    "   - Column data type detection (numeric, text, dates)\n",
    "   - Statistical summaries for numeric columns\n",
    "   - Missing value analysis with percentages\n",
    "   - Unique value counting for pattern detection\n",
    "\n",
    "3. **Invoice/Billing Detection**\n",
    "   - Automatically identifies invoice-related data\n",
    "   - Detects columns containing: invoice numbers, dates, amounts, vendors, etc.\n",
    "   - Flags sheets that likely contain billing information\n",
    "\n",
    "4. **Better Readability**\n",
    "   - Structured output with clear sections\n",
    "   - Metadata about file and sheet structure\n",
    "   - Preview of data with proper formatting\n",
    "   - Row and column summaries\n",
    "\n",
    "5. **Helper Functions**\n",
    "   - `analyze_excel_structure()` - Deep structural analysis\n",
    "   - `get_excel_summary()` - Quick overview with concise mode\n",
    "   - `get_excel_column_summary()` - Just column names (very lightweight)\n",
    "   - `read_excel_content()` - Full content extraction with metadata\n",
    "\n",
    "### âš¡ Optimization for Large Files\n",
    "\n",
    "**NEW**: The reader is now optimized to prevent token overflow errors:\n",
    "- Limits to **50 rows per sheet** by default (configurable)\n",
    "- Shows **first 3 sheets only** when reading all sheets\n",
    "- **Verbose mode disabled** by default (no detailed statistics)\n",
    "- **Auto-truncates** output if > 25,000 characters\n",
    "- Column width limited to 30 characters in preview\n",
    "\n",
    "**For large Excel files:**\n",
    "```python\n",
    "# Option 1: Read specific sheet only\n",
    "content = read_excel_content('large_file.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Option 2: Get just column structure (no data)\n",
    "columns = get_excel_column_summary('large_file.xlsx')\n",
    "\n",
    "# Option 3: Use concise summary\n",
    "summary = get_excel_summary('large_file.xlsx', concise=True)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "37650ed3-b2ec-43c3-8f97-42287718443d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# ============================================\n",
    "# Uncomment and run to install all dependencies\n",
    "\n",
    "# !pip install pdfplumber pandas openpyxl pillow pytesseract lxml\n",
    "# !pip install crewai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5f0ad",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for file reading and CrewAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e26bfd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "   â€¢ File readers: pdfplumber, pandas, PIL, pytesseract, xml\n",
      "   â€¢ CrewAI: Agent, Task, Crew, LLM\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORT ALL REQUIRED LIBRARIES\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# File reading libraries\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"   â€¢ File readers: pdfplumber, pandas, PIL, pytesseract, xml\")\n",
    "print(\"   â€¢ CrewAI: Agent, Task, Crew, LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "849307b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Individual file reader functions created!\n",
      "   â€¢ read_pdf_content()\n",
      "   â€¢ read_csv_content()\n",
      "   â€¢ read_excel_content() - OPTIMIZED for large files!\n",
      "   â€¢ read_image_content()\n",
      "   â€¢ read_xml_content()\n",
      "   â€¢ read_text_content()\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FILE READER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def read_pdf_content(pdf_path: str) -> str:\n",
    "    \"\"\"Read and extract text from PDF files.\"\"\"\n",
    "    try:\n",
    "        full_text = \"\"\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    full_text += f\"\\n--- Page {page_num} ---\\n{text}\\n\"\n",
    "        return full_text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_csv_content(csv_path: str) -> str:\n",
    "    \"\"\"Read and parse CSV files.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        return df.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading CSV: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_excel_content(excel_path: str, sheet_name=None, max_rows=50, verbose=False) -> str:\n",
    "    \"\"\"\n",
    "    Read and parse Excel files with enhanced capabilities.\n",
    "    \n",
    "    Features:\n",
    "    - Auto-detects and lists all sheets\n",
    "    - Reads multiple sheets or a specific sheet\n",
    "    - Provides data summary and structure\n",
    "    - Optimized for LLM consumption (prevents token overflow)\n",
    "    \n",
    "    Args:\n",
    "        excel_path: Path to the Excel file\n",
    "        sheet_name: Specific sheet name/index to read, or None for all sheets\n",
    "        max_rows: Maximum rows to display per sheet (default: 50, reduced for LLM)\n",
    "        verbose: If True, include detailed statistics (use False for large files)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with Excel content and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load Excel file to get sheet information\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        \n",
    "        output = []\n",
    "        output.append(f\"ðŸ“Š EXCEL FILE: {Path(excel_path).name}\")\n",
    "        output.append(f\"Total Sheets: {len(sheet_names)} - {', '.join(sheet_names)}\")\n",
    "        output.append(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Determine which sheets to read\n",
    "        sheets_to_read = []\n",
    "        if sheet_name is not None:\n",
    "            sheets_to_read = [sheet_name]\n",
    "        else:\n",
    "            # Read all sheets (limit to first 3 for large files to prevent token overflow)\n",
    "            sheets_to_read = sheet_names[:3]\n",
    "            if len(sheet_names) > 3:\n",
    "                output.append(f\"âš ï¸  Showing first 3 of {len(sheet_names)} sheets (to avoid data overflow)\\n\")\n",
    "        \n",
    "        # Read and analyze each sheet\n",
    "        for idx, sheet in enumerate(sheets_to_read, 1):\n",
    "            try:\n",
    "                df = pd.read_excel(excel_path, sheet_name=sheet)\n",
    "                \n",
    "                output.append(f\"\\nðŸ“„ SHEET {idx}: '{sheet}'\")\n",
    "                output.append(f\"{'-'*70}\")\n",
    "                output.append(f\"Size: {len(df)} rows Ã— {len(df.columns)} columns\")\n",
    "                \n",
    "                # Column info - summarized\n",
    "                output.append(f\"\\nColumns ({len(df.columns)}):\")\n",
    "                for col in df.columns:\n",
    "                    dtype = df[col].dtype\n",
    "                    non_null = df[col].notna().sum()\n",
    "                    unique = df[col].nunique()\n",
    "                    output.append(f\"  â€¢ {col}: {dtype} ({non_null} non-null, {unique} unique)\")\n",
    "                \n",
    "                # Determine how many rows to show\n",
    "                rows_to_show = min(max_rows, len(df))\n",
    "                \n",
    "                # Data preview - limited rows\n",
    "                output.append(f\"\\nData Preview (showing {rows_to_show} of {len(df)} rows):\")\n",
    "                output.append(df.head(rows_to_show).to_string(index=True, max_colwidth=30))\n",
    "                \n",
    "                # Only include detailed stats if verbose=True\n",
    "                if verbose:\n",
    "                    # Statistical summary for numeric columns\n",
    "                    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                    if len(numeric_cols) > 0:\n",
    "                        output.append(f\"\\nNumeric Summary:\")\n",
    "                        output.append(df[numeric_cols].describe().to_string())\n",
    "                    \n",
    "                    # Missing data\n",
    "                    missing_data = df.isnull().sum()\n",
    "                    if missing_data.any():\n",
    "                        output.append(f\"\\nMissing Values:\")\n",
    "                        for col, count in missing_data[missing_data > 0].items():\n",
    "                            pct = (count / len(df)) * 100\n",
    "                            output.append(f\"  â€¢ {col}: {count} ({pct:.1f}%)\")\n",
    "                else:\n",
    "                    # Just show totals for missing data\n",
    "                    total_missing = df.isnull().sum().sum()\n",
    "                    if total_missing > 0:\n",
    "                        output.append(f\"\\nTotal missing values: {total_missing}\")\n",
    "                \n",
    "                output.append(f\"\\n{'-'*70}\")\n",
    "                \n",
    "            except Exception as sheet_error:\n",
    "                output.append(f\"\\nâš ï¸  Error reading sheet '{sheet}': {str(sheet_error)}\\n\")\n",
    "        \n",
    "        result = \"\\n\".join(output)\n",
    "        \n",
    "        # Warn if output is very large\n",
    "        if len(result) > 25000:\n",
    "            return f\"âš ï¸ Excel file is very large ({len(result)} chars). Consider using sheet_name parameter.\\n\\n\" + result[:25000] + f\"\\n\\n... [Output truncated, use verbose=False or specify sheet_name]\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error reading Excel: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_image_content(image_path: str) -> str:\n",
    "    \"\"\"Extract text from images using OCR.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading image: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_xml_content(xml_path: str) -> str:\n",
    "    \"\"\"Parse and read XML files.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        return ET.tostring(root, encoding='unicode')\n",
    "    except Exception as e:\n",
    "        return f\"Error reading XML: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_text_content(text_path: str) -> str:\n",
    "    \"\"\"Read plain text files.\"\"\"\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading text file: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Individual file reader functions created!\")\n",
    "print(\"   â€¢ read_pdf_content()\")\n",
    "print(\"   â€¢ read_csv_content()\")\n",
    "print(\"   â€¢ read_excel_content() - OPTIMIZED for large files!\")\n",
    "print(\"   â€¢ read_image_content()\")\n",
    "print(\"   â€¢ read_xml_content()\")\n",
    "print(\"   â€¢ read_text_content()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b13ed92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel analysis helper functions created!\n",
      "   â€¢ analyze_excel_structure() - Deep Excel analysis\n",
      "   â€¢ get_excel_summary() - Quick summary (with concise mode)\n",
      "   â€¢ get_excel_column_summary() - Just column names (very lightweight)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXCEL-SPECIFIC HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def analyze_excel_structure(excel_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Deep analysis of Excel file structure.\n",
    "    Returns detailed information about sheets, data patterns, and potential invoice data.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results including:\n",
    "        - Sheet information\n",
    "        - Detected tables and data regions\n",
    "        - Potential header rows\n",
    "        - Column relationships\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        analysis = {\n",
    "            'file_name': Path(excel_path).name,\n",
    "            'total_sheets': len(excel_file.sheet_names),\n",
    "            'sheet_names': excel_file.sheet_names,\n",
    "            'sheets_analysis': {}\n",
    "        }\n",
    "        \n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "            \n",
    "            # Analyze this sheet\n",
    "            sheet_info = {\n",
    "                'rows': len(df),\n",
    "                'columns': len(df.columns),\n",
    "                'column_names': list(df.columns),\n",
    "                'has_unnamed_columns': any('Unnamed' in str(col) for col in df.columns),\n",
    "                'numeric_columns': list(df.select_dtypes(include=['number']).columns),\n",
    "                'text_columns': list(df.select_dtypes(include=['object']).columns),\n",
    "                'date_columns': list(df.select_dtypes(include=['datetime']).columns),\n",
    "                'empty_rows': df.isnull().all(axis=1).sum(),\n",
    "                'duplicate_rows': df.duplicated().sum(),\n",
    "            }\n",
    "            \n",
    "            # Try to detect if this looks like invoice/billing data\n",
    "            invoice_indicators = []\n",
    "            keywords = ['invoice', 'bill', 'amount', 'total', 'price', 'quantity', 'date', 'vendor', 'payment']\n",
    "            \n",
    "            for col in df.columns:\n",
    "                col_str = str(col).lower()\n",
    "                for keyword in keywords:\n",
    "                    if keyword in col_str:\n",
    "                        invoice_indicators.append(f\"Column '{col}' contains '{keyword}'\")\n",
    "            \n",
    "            # Check for data in cells\n",
    "            for col in df.columns:\n",
    "                sample_values = df[col].dropna().astype(str).head(5).tolist()\n",
    "                for keyword in keywords:\n",
    "                    if any(keyword in str(val).lower() for val in sample_values):\n",
    "                        invoice_indicators.append(f\"Column '{col}' has values matching '{keyword}'\")\n",
    "                        break\n",
    "            \n",
    "            sheet_info['invoice_indicators'] = invoice_indicators\n",
    "            sheet_info['likely_invoice_data'] = len(invoice_indicators) > 0\n",
    "            \n",
    "            analysis['sheets_analysis'][sheet_name] = sheet_info\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "def get_excel_summary(excel_path: str, concise=True) -> str:\n",
    "    \"\"\"\n",
    "    Get a summary of Excel file suitable for LLM understanding.\n",
    "    \n",
    "    Args:\n",
    "        excel_path: Path to Excel file\n",
    "        concise: If True, returns brief summary. If False, more detailed.\n",
    "    \"\"\"\n",
    "    analysis = analyze_excel_structure(excel_path)\n",
    "    \n",
    "    if 'error' in analysis:\n",
    "        return f\"Error analyzing Excel file: {analysis['error']}\"\n",
    "    \n",
    "    summary = []\n",
    "    summary.append(f\"ðŸ“Š Excel File: {analysis['file_name']}\")\n",
    "    summary.append(f\"Total Sheets: {analysis['total_sheets']}\")\n",
    "    \n",
    "    if concise:\n",
    "        # Brief summary\n",
    "        summary.append(f\"Sheets: {', '.join(analysis['sheet_names'])}\")\n",
    "        total_rows = sum(info['rows'] for info in analysis['sheets_analysis'].values())\n",
    "        summary.append(f\"Total Data Rows: {total_rows}\")\n",
    "    else:\n",
    "        # Detailed summary\n",
    "        summary.append(\"\")\n",
    "        for sheet_name, info in analysis['sheets_analysis'].items():\n",
    "            summary.append(f\"Sheet: {sheet_name}\")\n",
    "            summary.append(f\"  Size: {info['rows']} rows Ã— {info['columns']} columns\")\n",
    "            summary.append(f\"  Columns: {', '.join([str(c) for c in info['column_names'][:10]])}\")\n",
    "            \n",
    "            if info['likely_invoice_data']:\n",
    "                summary.append(f\"  âœ… Likely contains billing/invoice data\")\n",
    "                summary.append(f\"  Indicators: {'; '.join(info['invoice_indicators'][:3])}\")\n",
    "            \n",
    "            summary.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def get_excel_column_summary(excel_path: str, sheet_name=None) -> str:\n",
    "    \"\"\"\n",
    "    Get just the column names and types from an Excel file.\n",
    "    Useful for understanding structure without loading all data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        sheets = [sheet_name] if sheet_name else excel_file.sheet_names[:3]\n",
    "        \n",
    "        summary = []\n",
    "        summary.append(f\"ðŸ“‹ Column Structure: {Path(excel_path).name}\")\n",
    "        \n",
    "        for sheet in sheets:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet, nrows=0)  # Just headers\n",
    "            summary.append(f\"\\nSheet: {sheet}\")\n",
    "            summary.append(f\"Columns: {', '.join([str(c) for c in df.columns])}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Excel analysis helper functions created!\")\n",
    "print(\"   â€¢ analyze_excel_structure() - Deep Excel analysis\")\n",
    "print(\"   â€¢ get_excel_summary() - Quick summary (with concise mode)\")\n",
    "print(\"   â€¢ get_excel_column_summary() - Just column names (very lightweight)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a23184b0-3a10-4523-982d-7df0b66593da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CrewAI File Reader Tool created!\n",
      "   Tool name: 'read_file_tool'\n",
      "   Supports: PDF, CSV, Excel (Optimized!), Images, XML, Text\n",
      "   Excel Features: Auto-limited to 50 rows/sheet to prevent token overflow\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIVERSAL FILE READER TOOL (CrewAI Tool)\n",
    "# ============================================\n",
    "\n",
    "@tool(\"read_file_tool\")\n",
    "def read_file_tool(file_path: str, sheet_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Universal file reader tool that automatically detects and reads various file formats.\n",
    "    Supports: PDF, CSV, Excel (with multi-sheet support), Images (OCR), XML, and Text files.\n",
    "    \n",
    "    OPTIMIZED for AI agents - prevents token overflow on large Excel files.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        sheet_name: (Optional) For Excel files, specify sheet name or index to read.\n",
    "                   If None, reads first 3 sheets. Examples: \"Sheet1\", 0, \"Invoice Data\"\n",
    "        \n",
    "    Returns:\n",
    "        Extracted content as string with detailed formatting and metadata\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return f\"âŒ Error: File not found at {file_path}\"\n",
    "    \n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    print(f\"ðŸ“‚ Reading file: {file_path.name} (type: {suffix})\")\n",
    "    \n",
    "    # Route to appropriate reader based on file extension\n",
    "    if suffix == '.pdf':\n",
    "        content = read_pdf_content(str(file_path))\n",
    "    elif suffix == '.csv':\n",
    "        content = read_csv_content(str(file_path))\n",
    "    elif suffix in ['.xlsx', '.xls']:\n",
    "        # Use enhanced Excel reader with optimization for LLM\n",
    "        print(f\"   Using optimized Excel reader (max 50 rows/sheet)...\")\n",
    "        if sheet_name:\n",
    "            print(f\"   Reading specific sheet: {sheet_name}\")\n",
    "        # Use verbose=False to prevent token overflow\n",
    "        content = read_excel_content(str(file_path), sheet_name=sheet_name, max_rows=50, verbose=False)\n",
    "    elif suffix in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "        content = read_image_content(str(file_path))\n",
    "    elif suffix == '.xml':\n",
    "        content = read_xml_content(str(file_path))\n",
    "    elif suffix == '.txt':\n",
    "        content = read_text_content(str(file_path))\n",
    "    else:\n",
    "        content = f\"âš ï¸ Unsupported file type: {suffix}\"\n",
    "    \n",
    "    print(f\"âœ… Successfully read {len(content)} characters\")\n",
    "    return content\n",
    "\n",
    "print(\"âœ… CrewAI File Reader Tool created!\")\n",
    "print(\"   Tool name: 'read_file_tool'\")\n",
    "print(\"   Supports: PDF, CSV, Excel (Optimized!), Images, XML, Text\")\n",
    "print(\"   Excel Features: Auto-limited to 50 rows/sheet to prevent token overflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4a6063a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel metadata tool created!\n",
      "   Tool name: 'get_excel_info'\n",
      "   Purpose: Lightweight Excel file inspection (minimal tokens)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ADDITIONAL HELPER TOOL FOR EXCEL METADATA\n",
    "# ============================================\n",
    "\n",
    "@tool(\"get_excel_info\")\n",
    "def get_excel_info(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Get lightweight metadata about an Excel file without loading all data.\n",
    "    Use this FIRST before reading full Excel content to understand structure.\n",
    "    \n",
    "    Returns:\n",
    "    - File name\n",
    "    - Total sheets and their names\n",
    "    - Total rows across all sheets\n",
    "    - Brief indication if likely contains invoice/billing data\n",
    "    \n",
    "    This is very fast and uses minimal tokens.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return f\"âŒ Error: File not found at {file_path}\"\n",
    "    \n",
    "    if file_path.suffix.lower() not in ['.xlsx', '.xls']:\n",
    "        return f\"âš ï¸ Not an Excel file: {file_path.suffix}\"\n",
    "    \n",
    "    return get_excel_summary(str(file_path), concise=True)\n",
    "\n",
    "print(\"âœ… Excel metadata tool created!\")\n",
    "print(\"   Tool name: 'get_excel_info'\")\n",
    "print(\"   Purpose: Lightweight Excel file inspection (minimal tokens)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a19f8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key loaded\n",
      "âœ… LLM initialized: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD ENVIRONMENT AND INITIALIZE LLM\n",
    "# ============================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found in environment\")\n",
    "    print(\"   Please add it to your .env file\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key loaded\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM initialized: gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cd34681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File Reader Agent created!\n",
      "   Role: File Data Extraction Specialist\n",
      "   Tools: ['get_excel_info', 'read_file_tool']\n",
      "   âš¡ Optimized for large Excel files!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE FILE READER AGENT\n",
    "# ============================================\n",
    "\n",
    "file_reader_agent = Agent(\n",
    "    role='File Data Extraction Specialist',\n",
    "    goal='Read and extract data from various file formats including PDF, CSV, Excel, images, XML, and text files',\n",
    "    backstory=\"\"\"You are an expert in data extraction and file parsing. \n",
    "    You can read any type of document - from PDFs and spreadsheets to scanned images \n",
    "    and XML files. You extract information accurately and structure it in a clear, \n",
    "    usable format. You handle invoices, billing documents, and financial records \n",
    "    with precision.\n",
    "    \n",
    "    IMPORTANT: For Excel files, always use get_excel_info FIRST to understand \n",
    "    the file structure before reading full content. This prevents token overflow.\"\"\",\n",
    "    llm=llm,\n",
    "    tools=[get_excel_info, read_file_tool],\n",
    "    verbose=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "print(\"âœ… File Reader Agent created!\")\n",
    "print(f\"   Role: {file_reader_agent.role}\")\n",
    "print(f\"   Tools: {[tool.name for tool in file_reader_agent.tools]}\")\n",
    "print(\"   âš¡ Optimized for large Excel files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d374e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File reading task creator function defined!\n",
      "   Usage: task = create_file_reading_task('path/to/file.pdf')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE FILE READING TASK\n",
    "# ============================================\n",
    "\n",
    "def create_file_reading_task(file_path: str, extraction_focus: str = \"all data\") -> Task:\n",
    "    \"\"\"\n",
    "    Create a task for reading and extracting data from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        extraction_focus: What specific data to focus on (default: \"all data\")\n",
    "    \n",
    "    Returns:\n",
    "        Task object configured for file reading\n",
    "    \"\"\"\n",
    "    task = Task(\n",
    "        description=f\"\"\"Read and extract data from the following file:\n",
    "        \n",
    "File Path: {file_path}\n",
    "Extraction Focus: {extraction_focus}\n",
    "\n",
    "Your tasks:\n",
    "1. Use the read_file_tool to read the file\n",
    "2. Extract and structure the relevant information\n",
    "3. If it's an invoice or billing document, identify:\n",
    "   - Vendor/supplier name\n",
    "   - Invoice number and date\n",
    "   - Line items with descriptions and amounts\n",
    "   - Total amount\n",
    "   - Any other relevant billing information\n",
    "4. Present the extracted data in a clear, structured format\n",
    "\n",
    "Provide a comprehensive summary of the file contents.\"\"\",\n",
    "        agent=file_reader_agent,\n",
    "        expected_output=\"\"\"A structured summary of the file contents including:\n",
    "        - File type and basic metadata\n",
    "        - Key data points extracted\n",
    "        - For invoices: vendor, invoice number, date, line items, total\n",
    "        - Any notable information or patterns found\"\"\"\n",
    "    )\n",
    "    \n",
    "    return task\n",
    "\n",
    "print(\"âœ… File reading task creator function defined!\")\n",
    "print(\"   Usage: task = create_file_reading_task('path/to/file.pdf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c8aed",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Define Canonical Invoice Schema\n",
    "\n",
    "This schema defines the standard structure for extracted invoice data with clear validation rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d06ea140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Canonical Invoice Schema defined!\n",
      "   - Standard JSON structure for all invoice types\n",
      "   - Clear extraction rules for discounts, revenue, profit\n",
      "   - Handles PDF, Excel, CSV, and text formats\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CANONICAL INVOICE SCHEMA & EXTRACTION RULES\n",
    "# ============================================\n",
    "\n",
    "CANONICAL_SCHEMA_DOC = \"\"\"\n",
    "You MUST output a JSON object matching this EXACT structure:\n",
    "\n",
    "{\n",
    "  \"invoice_header\": {\n",
    "    \"invoice_number\": string or null,\n",
    "    \"vendor_name\": string or null,\n",
    "    \"campaign_name\": string or null,\n",
    "    \"invoice_date\": string or null,          // YYYY-MM-DD if possible\n",
    "    \"billing_start_date\": string or null,    // YYYY-MM-DD if possible\n",
    "    \"billing_end_date\": string or null,      // YYYY-MM-DD if possible\n",
    "    \"currency\": string or null,\n",
    "    \"total_impressions\": number or null,\n",
    "    \"total_views\": number or null,           // complete views, video views, conversions, etc.\n",
    "    \"total_clicks\": number or null,\n",
    "    \"gross_revenue\": number or null,\n",
    "    \"net_revenue\": number or null,\n",
    "    \"total_discount_amount\": number or null,\n",
    "    \"discount_percent\": number or null,\n",
    "    \"profit\": number or null\n",
    "  },\n",
    "  \"line_items\": [\n",
    "    {\n",
    "      \"line_id\": integer,\n",
    "      \"campaign_name\": string or null,\n",
    "      \"placement\": string or null,\n",
    "      \"start_date\": string or null,          // YYYY-MM-DD if possible\n",
    "      \"end_date\": string or null,            // YYYY-MM-DD if possible\n",
    "      \"planned_impressions\": number or null,\n",
    "      \"billed_impressions\": number or null,\n",
    "      \"views\": number or null,               // complete views, video views, conversions, etc.\n",
    "      \"clicks\": number or null,\n",
    "      \"gross_revenue\": number or null,\n",
    "      \"net_revenue\": number or null,\n",
    "      \"discount_amount\": number or null,\n",
    "      \"discount_percent\": number or null,\n",
    "      \"profit\": number or null,\n",
    "      \"rate_type\": string or null,           // CPM, CPC, CPV, Flat, etc.\n",
    "      \"rate\": number or null\n",
    "    }\n",
    "  ],\n",
    "  \"notes\": string or null                     // Any additional context or clarifications\n",
    "}\n",
    "\n",
    "EXTRACTION RULES:\n",
    "1. If a value is not present in the invoice, use null - DO NOT INVENT DATA.\n",
    "2. If only one type of revenue is present, store it in gross_revenue and leave net_revenue null.\n",
    "3. Discounts can be:\n",
    "   - Explicit: directly stated as \"discount\" column\n",
    "   - Implicit: difference between gross_revenue and net_revenue\n",
    "   - Explain in 'notes' field if inferred\n",
    "4. Profit calculation:\n",
    "   - If stated directly, use that value\n",
    "   - Otherwise: profit = net_revenue - cost (if cost is available)\n",
    "   - If cannot be determined, leave as null\n",
    "5. Views/Impressions:\n",
    "   - Use the exact metric name from the invoice\n",
    "   - Map \"complete views\", \"video views\", \"conversions\" to the closest field\n",
    "   - Explain mapping in notes if ambiguous\n",
    "6. Dates:\n",
    "   - Convert to YYYY-MM-DD format when possible\n",
    "   - Keep original format in notes if conversion is uncertain\n",
    "7. Currency:\n",
    "   - Extract currency code (USD, EUR, GBP, etc.) from invoice\n",
    "8. Line Items:\n",
    "   - Each row in a tabular invoice becomes one line_item\n",
    "   - Assign sequential line_id starting from 1\n",
    "9. Aggregation:\n",
    "   - invoice_header totals should sum up from line_items when not explicitly stated\n",
    "   - Flag any discrepancies in notes\n",
    "\n",
    "BE CONSERVATIVE: Do not invent numbers. Use null for missing data.\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Canonical Invoice Schema defined!\")\n",
    "print(\"   - Standard JSON structure for all invoice types\")\n",
    "print(\"   - Clear extraction rules for discounts, revenue, profit\")\n",
    "print(\"   - Handles PDF, Excel, CSV, and text formats\")\n",
    "CANONICAL_SCHEMA_DOC = \"\"\"\n",
    "You MUST output JSON ARRAY matching this structure:\n",
    " \n",
    "[\n",
    "  \"invoice_header\": {\n",
    "    \"invoice_number\": string or null,\n",
    "    \"vendor_name\": string or null,\n",
    "    \"campaign_name\": string or null,\n",
    "    \"invoice_date\": string or null,          // YYYY-MM-DD if possible\n",
    "    \"billing_start_date\": string or null,    // YYYY-MM-DD if possible\n",
    "    \"billing_end_date\": string or null,      // YYYY-MM-DD if possible\n",
    "    \"currency\": string or null,           // YYYY-MM-DD if possible\n",
    "      \"impressions\": number or null,\n",
    "      \"views\": number or null,               // complete views, video views, completed clicks, clicks conversions, etc. choose the closest\n",
    "      \"gross_revenue\": number or null,\n",
    "      \"net_revenue\": number or null,\n",
    "      \"discount_amount\": number or null,\n",
    "      \"discount_percent\": number or null,\n",
    "      \"profit\": number or null,\n",
    "      \"line_items\": [\n",
    "        {\n",
    "          \"line_id\": integer,\n",
    "          \"campaign_name\": string or null,\n",
    "          \"placement\": string or null,\n",
    "          \"start_date\": string or null,          // YYYY-MM-DD if possible\n",
    "          \"end_date\": string or null,            // YYYY-MM-DD if possible\n",
    "          \"planned impressions\": number or null,\n",
    "          \"billed impressions\": number or null,\n",
    "          \"views\": number or null,               // complete views, video views, completed clicks, clicks conversions, etc. choose the closest\n",
    "          \"gross_revenue\": number or null,\n",
    "          \"net_revenue\": number or null,\n",
    "          \"discount_amount\": number or null,\n",
    "          \"discount_percent\": number or null,\n",
    "          \"profit\": number or null,\n",
    "          \"rate_type\": string or null,           // CPM, CPC, CPV, Flat, etc.\n",
    "          \"rate\": number or null,\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "  ]\n",
    " \n",
    "RULES:\n",
    "- If a value is not present in the invoice, use null.\n",
    "- If only one type of revenue is present, store it in gross_revenue and leave net_revenue null (or vice versa if clearly net).\n",
    "- Discounts can be explicit (discount column) or implicit (difference between gross and net) â€” explain in notes if inferred.\n",
    "- Profit = revenue - cost, if not directly provided.\n",
    "- Be conservative: do NOT invent numbers if they are not in the invoice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3c8baadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Invoice context builder functions created!\n",
      "   â€¢ build_invoice_context() - Unified file type handler\n",
      "   â€¢ format_context_for_agent() - Format for LLM consumption\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENHANCED INVOICE CONTEXT BUILDER\n",
    "# ============================================\n",
    "\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def build_invoice_context(\n",
    "    file_path: str,\n",
    "    max_rows: int = 30\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a unified context object from PDF/Excel/CSV for the agent.\n",
    "    Returns structured context with type detection and preview.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the invoice file\n",
    "        max_rows: Maximum rows to include in preview (for tables)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with invoice context including file type and content\n",
    "    \"\"\"\n",
    "    file_path_obj = Path(file_path)\n",
    "    \n",
    "    if not file_path_obj.exists():\n",
    "        return {\"error\": f\"File not found: {file_path}\"}\n",
    "    \n",
    "    suffix = file_path_obj.suffix.lower()\n",
    "    \n",
    "    context = {\n",
    "        \"file_path\": str(file_path),\n",
    "        \"file_name\": file_path_obj.name,\n",
    "        \"input_type\": None,\n",
    "        \"content\": None\n",
    "    }\n",
    "    \n",
    "    # PDF files\n",
    "    if suffix == '.pdf':\n",
    "        context[\"input_type\"] = \"pdf\"\n",
    "        # Read PDF text\n",
    "        pdf_text = read_pdf_content(str(file_path))\n",
    "        context[\"raw_text\"] = pdf_text\n",
    "        context[\"preview\"] = pdf_text[:2000] + \"...\" if len(pdf_text) > 2000 else pdf_text\n",
    "    \n",
    "    # Excel/CSV files\n",
    "    elif suffix in ['.xlsx', '.xls', '.csv']:\n",
    "        context[\"input_type\"] = \"excel\" if suffix in ['.xlsx', '.xls'] else \"csv\"\n",
    "        \n",
    "        # Read as DataFrame\n",
    "        if suffix in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = [\n",
    "            str(c).strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            for c in df.columns\n",
    "        ]\n",
    "        \n",
    "        # Create table preview\n",
    "        preview_df = df.head(max_rows)\n",
    "        \n",
    "        context[\"table_preview\"] = {\n",
    "            \"total_rows\": len(df),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"column_types\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            \"preview_rows\": preview_df.to_dict(orient=\"records\"),\n",
    "            \"preview_text\": preview_df.to_string(index=False)\n",
    "        }\n",
    "    \n",
    "    # Text files\n",
    "    elif suffix == '.txt':\n",
    "        context[\"input_type\"] = \"text\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        context[\"raw_text\"] = text\n",
    "        context[\"preview\"] = text[:2000] + \"...\" if len(text) > 2000 else text\n",
    "    \n",
    "    else:\n",
    "        context[\"error\"] = f\"Unsupported file type: {suffix}\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def format_context_for_agent(context: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format the invoice context into a readable string for the agent.\n",
    "    \"\"\"\n",
    "    if \"error\" in context:\n",
    "        return f\"ERROR: {context['error']}\"\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"FILE: {context['file_name']}\")\n",
    "    output.append(f\"TYPE: {context['input_type'].upper()}\")\n",
    "    output.append(\"=\" * 70)\n",
    "    \n",
    "    if context[\"input_type\"] in [\"pdf\", \"text\"]:\n",
    "        output.append(\"\\nRAW TEXT CONTENT:\")\n",
    "        output.append(context.get(\"raw_text\", \"\"))\n",
    "    \n",
    "    elif context[\"input_type\"] in [\"excel\", \"csv\"]:\n",
    "        table_preview = context.get(\"table_preview\", {})\n",
    "        output.append(f\"\\nTABLE STRUCTURE:\")\n",
    "        output.append(f\"Total Rows: {table_preview.get('total_rows', 0)}\")\n",
    "        output.append(f\"Columns: {', '.join(table_preview.get('columns', []))}\")\n",
    "        output.append(f\"\\nDATA PREVIEW (first {len(table_preview.get('preview_rows', []))} rows):\")\n",
    "        output.append(table_preview.get(\"preview_text\", \"\"))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "print(\"âœ… Invoice context builder functions created!\")\n",
    "print(\"   â€¢ build_invoice_context() - Unified file type handler\")\n",
    "print(\"   â€¢ format_context_for_agent() - Format for LLM consumption\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6add718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced invoice extraction task creator defined!\n",
      "   Usage: task = create_invoice_extraction_task('invoice.xlsx')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENHANCED INVOICE EXTRACTION TASK\n",
    "# ============================================\n",
    "\n",
    "def create_invoice_extraction_task(file_path: str, max_rows: int = 30) -> Task:\n",
    "    \"\"\"\n",
    "    Create a task for extracting structured invoice data using the canonical schema.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the invoice file (PDF, Excel, CSV, or text)\n",
    "        max_rows: Maximum rows to include from tabular data\n",
    "    \n",
    "    Returns:\n",
    "        Task object configured for invoice feature extraction\n",
    "    \"\"\"\n",
    "    # Build context from file\n",
    "    context = build_invoice_context(file_path, max_rows=max_rows)\n",
    "    context_str = format_context_for_agent(context)\n",
    "    \n",
    "    description = f\"\"\"\n",
    "You are analyzing a media/advertising invoice to extract key financial and delivery features.\n",
    "\n",
    "**YOUR TASK:**\n",
    "1. Understand the invoice header (vendor, invoice number, campaign, dates, currency)\n",
    "2. Identify all line items (placements, quantities, prices, metrics)\n",
    "3. Extract key features:\n",
    "   - Discounts (amount and/or percent - explicit or implicit)\n",
    "   - Impressions (planned vs. billed)\n",
    "   - Views (complete views, video views, or nearest equivalent)\n",
    "   - Clicks\n",
    "   - Revenues (gross and net)\n",
    "   - Costs\n",
    "   - Profits\n",
    "4. Map everything into the canonical JSON schema\n",
    "\n",
    "**CANONICAL SCHEMA:**\n",
    "{CANONICAL_SCHEMA_DOC}\n",
    "\n",
    "**INVOICE DATA:**\n",
    "{context_str}\n",
    "\n",
    "**IMPORTANT INSTRUCTIONS:**\n",
    "- Carefully scan all available fields and column names\n",
    "- Look for columns like: impressions, views, complete_views, revenue, cost, profit, discount, etc.\n",
    "- If only some metrics exist, fill those and leave the rest as null\n",
    "- For Excel/CSV: each data row becomes one line_item with sequential line_id\n",
    "- For PDF: extract line items from tables or structured text\n",
    "- If discounts or profit need to be inferred (e.g., revenue - cost), compute it and explain in 'notes'\n",
    "- Dates should be in YYYY-MM-DD format when possible\n",
    "- Return ONLY the JSON object - no markdown, no explanations outside of JSON\n",
    "- DO NOT INVENT DATA - use null for missing values\n",
    "\n",
    "**OUTPUT:**\n",
    "Return a single valid JSON object following the canonical schema exactly.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    task = Task(\n",
    "        description=description,\n",
    "        agent=file_reader_agent,\n",
    "        expected_output=\"\"\"A single valid JSON object with:\n",
    "        - invoice_header: all header-level fields\n",
    "        - line_items: array of line item objects\n",
    "        - notes: any clarifications or assumptions made\n",
    "        \n",
    "        The JSON must be valid and parseable.\"\"\"\n",
    "    )\n",
    "    \n",
    "    return task\n",
    "\n",
    "\n",
    "print(\"âœ… Enhanced invoice extraction task creator defined!\")\n",
    "print(\"   Usage: task = create_invoice_extraction_task('invoice.xlsx')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4033585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Main invoice feature extraction function created!\n",
      "   Usage: result = extract_invoice_features('invoice.xlsx')\n",
      "   Returns: Structured JSON with invoice_header, line_items, and notes\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MAIN INVOICE FEATURE EXTRACTION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def extract_invoice_features(file_path: str, max_rows: int = 30) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured invoice features using AI agent with canonical schema.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to invoice file (PDF, Excel, CSV, or text)\n",
    "        max_rows: Maximum rows to process from tabular files\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extracted invoice data in canonical format\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“„ Extracting Invoice Features: {Path(file_path).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create extraction task\n",
    "    task = create_invoice_extraction_task(file_path, max_rows=max_rows)\n",
    "    \n",
    "    # Create crew\n",
    "    crew = Crew(\n",
    "        agents=[file_reader_agent],\n",
    "        tasks=[task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Execute\n",
    "    raw_result = crew.kickoff()\n",
    "    result_str = str(raw_result).strip()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… Extraction Complete!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Try to parse JSON\n",
    "    try:\n",
    "        # First attempt: direct parse\n",
    "        parsed = json.loads(result_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: find JSON object in the response\n",
    "        print(\"âš ï¸  Direct JSON parse failed, attempting to extract JSON...\")\n",
    "        start = result_str.find(\"{\")\n",
    "        end = result_str.rfind(\"}\")\n",
    "        \n",
    "        if start != -1 and end != -1 and start < end:\n",
    "            json_str = result_str[start : end + 1]\n",
    "            try:\n",
    "                parsed = json.loads(json_str)\n",
    "                print(\"âœ… Successfully extracted JSON from response\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âŒ JSON parsing failed: {e}\")\n",
    "                return {\n",
    "                    \"error\": \"Failed to parse JSON response\",\n",
    "                    \"raw_response\": result_str[:500]\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                \"error\": \"No JSON object found in response\",\n",
    "                \"raw_response\": result_str[:500]\n",
    "            }\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "\n",
    "print(\"âœ… Main invoice feature extraction function created!\")\n",
    "print(\"   Usage: result = extract_invoice_features('invoice.xlsx')\")\n",
    "print(\"   Returns: Structured JSON with invoice_header, line_items, and notes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7202680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Validation and export helper functions created!\n",
      "   â€¢ validate_invoice_result() - Check schema compliance\n",
      "   â€¢ save_invoice_json() - Save to JSON file\n",
      "   â€¢ invoice_to_dataframe() - Convert to pandas DataFrame\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESULT VALIDATION & EXPORT HELPERS\n",
    "# ============================================\n",
    "\n",
    "def validate_invoice_result(result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the extracted invoice data against the canonical schema.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation status and any issues found\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        \"valid\": True,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": []\n",
    "    }\n",
    "    \n",
    "    # Check for top-level structure\n",
    "    if \"invoice_header\" not in result:\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"Missing 'invoice_header' field\")\n",
    "    \n",
    "    if \"line_items\" not in result:\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"Missing 'line_items' field\")\n",
    "    elif not isinstance(result[\"line_items\"], list):\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"'line_items' must be an array\")\n",
    "    \n",
    "    # Check invoice_header fields\n",
    "    if \"invoice_header\" in result:\n",
    "        header = result[\"invoice_header\"]\n",
    "        if not header.get(\"invoice_number\") and not header.get(\"vendor_name\"):\n",
    "            validation[\"warnings\"].append(\"Missing both invoice_number and vendor_name in header\")\n",
    "    \n",
    "    # Check line_items structure\n",
    "    if \"line_items\" in result and isinstance(result[\"line_items\"], list):\n",
    "        for idx, item in enumerate(result[\"line_items\"]):\n",
    "            if \"line_id\" not in item:\n",
    "                validation[\"warnings\"].append(f\"Line item {idx} missing 'line_id'\")\n",
    "    \n",
    "    return validation\n",
    "\n",
    "\n",
    "def save_invoice_json(result: Dict[str, Any], output_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save the extracted invoice data to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        result: Extracted invoice data\n",
    "        output_path: Path to save JSON (optional, auto-generated if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        # Auto-generate filename\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"data/extracted_invoice_{timestamp}.json\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def invoice_to_dataframe(result: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert extracted invoice line items to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        result: Extracted invoice data with line_items\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all line items\n",
    "    \"\"\"\n",
    "    if \"line_items\" not in result or not result[\"line_items\"]:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(result[\"line_items\"])\n",
    "    \n",
    "    # Add header information to each row\n",
    "    if \"invoice_header\" in result:\n",
    "        header = result[\"invoice_header\"]\n",
    "        df[\"invoice_number\"] = header.get(\"invoice_number\")\n",
    "        df[\"vendor_name\"] = header.get(\"vendor_name\")\n",
    "        df[\"invoice_date\"] = header.get(\"invoice_date\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"âœ… Validation and export helper functions created!\")\n",
    "print(\"   â€¢ validate_invoice_result() - Check schema compliance\")\n",
    "print(\"   â€¢ save_invoice_json() - Save to JSON file\")\n",
    "print(\"   â€¢ invoice_to_dataframe() - Convert to pandas DataFrame\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b47f4a",
   "metadata": {},
   "source": [
    "## ðŸš€ Example: Extract Invoice Features with Canonical Schema\n",
    "\n",
    "Now let's test the enhanced invoice extraction with the improved prompts and schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE: EXTRACT FROM SAMPLE INVOICE\n",
    "# ============================================\n",
    "\n",
    "# Extract features from the sample invoice we created earlier\n",
    "result = extract_invoice_features('data/sample_media_invoice.xlsx', max_rows=50)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š EXTRACTED INVOICE DATA\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# Validate the result\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "validation = validate_invoice_result(result)\n",
    "print(f\"Valid: {validation['valid']}\")\n",
    "if validation['errors']:\n",
    "    print(f\"Errors: {validation['errors']}\")\n",
    "if validation['warnings']:\n",
    "    print(f\"Warnings: {validation['warnings']}\")\n",
    "\n",
    "# Save to JSON\n",
    "output_file = save_invoice_json(result, \"data/extracted_sample_invoice.json\")\n",
    "print(f\"\\nðŸ’¾ Saved to: {output_file}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = invoice_to_dataframe(result)\n",
    "print(f\"\\nðŸ“‹ DataFrame Preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63f023",
   "metadata": {},
   "source": [
    "## ðŸ“„ Process Your Real Invoice File\n",
    "\n",
    "Use this cell to process your actual invoice file with the enhanced extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROCESS YOUR INVOICE FILE\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Check the file structure first (optional but recommended)\n",
    "print(\"ðŸ“‹ File Structure Analysis:\")\n",
    "summary = get_excel_summary('data/All Raw data.xlsx', concise=True)\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 2: Extract invoice features with canonical schema\n",
    "# Note: This will process the first sheet with up to 50 rows\n",
    "# Adjust max_rows as needed, but keep it reasonable to avoid token limits\n",
    "\n",
    "invoice_data = extract_invoice_features(\n",
    "    'data/All Raw data.xlsx',\n",
    "    max_rows=50  # Limit rows to prevent token overflow\n",
    ")\n",
    "\n",
    "# Step 3: Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š EXTRACTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if \"error\" in invoice_data:\n",
    "    print(f\"âŒ Error: {invoice_data['error']}\")\n",
    "else:\n",
    "    print(\"\\nðŸ“‹ Invoice Header:\")\n",
    "    print(json.dumps(invoice_data.get(\"invoice_header\", {}), indent=2))\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Line Items: {len(invoice_data.get('line_items', []))}\")\n",
    "    if invoice_data.get('line_items'):\n",
    "        print(\"\\nFirst 3 line items:\")\n",
    "        for item in invoice_data['line_items'][:3]:\n",
    "            print(f\"  Line {item.get('line_id')}: {item.get('campaign_name', 'N/A')}\")\n",
    "            print(f\"    Revenue: {item.get('gross_revenue', 'N/A')}, Impressions: {item.get('billed_impressions', 'N/A')}\")\n",
    "    \n",
    "    if invoice_data.get('notes'):\n",
    "        print(f\"\\nðŸ“ Notes: {invoice_data['notes']}\")\n",
    "    \n",
    "    # Step 4: Validate\n",
    "    validation = validate_invoice_result(invoice_data)\n",
    "    print(f\"\\nâœ… Validation: {'PASS' if validation['valid'] else 'FAIL'}\")\n",
    "    if validation['warnings']:\n",
    "        print(f\"âš ï¸  Warnings: {', '.join(validation['warnings'])}\")\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    output_file = save_invoice_json(invoice_data, \"data/extracted_invoice_features.json\")\n",
    "    print(f\"\\nðŸ’¾ Saved to: {output_file}\")\n",
    "    \n",
    "    # Step 6: Convert to DataFrame for analysis\n",
    "    df = invoice_to_dataframe(invoice_data)\n",
    "    if not df.empty:\n",
    "        print(f\"\\nðŸ“Š DataFrame created with {len(df)} rows\")\n",
    "        print(\"\\nSample data:\")\n",
    "        print(df[['line_id', 'campaign_name', 'gross_revenue', 'billed_impressions']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062170f",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 6: Create and Run Crew\n",
    "\n",
    "Assemble the crew and execute the file reading task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "952f3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File processing function created!\n",
      "   Usage: result = process_file('path/to/invoice.pdf')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE AND RUN FILE READER CREW\n",
    "# ============================================\n",
    "\n",
    "def process_file(file_path: str, extraction_focus: str = \"all data\"):\n",
    "    \"\"\"\n",
    "    Process a file using the File Reader Agent and Crew.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to process\n",
    "        extraction_focus: Specific data to focus on\n",
    "        \n",
    "    Returns:\n",
    "        Extracted data and analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ Processing File: {Path(file_path).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create task for this file\n",
    "    task = create_file_reading_task(file_path, extraction_focus)\n",
    "    \n",
    "    # Create crew\n",
    "    crew = Crew(\n",
    "        agents=[file_reader_agent],\n",
    "        tasks=[task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Execute\n",
    "    result = crew.kickoff()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… File Processing Complete!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… File processing function created!\")\n",
    "print(\"   Usage: result = process_file('path/to/invoice.pdf')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
