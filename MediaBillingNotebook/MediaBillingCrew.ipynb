{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef392679-3088-45dd-b075-93fc41422e81",
   "metadata": {},
   "source": [
    "# MEDIA BILLING RECONCILIATION WITH AGENTIC AIâ€‹\n",
    "\n",
    "---\n",
    "\n",
    "## DESCRIPTION\n",
    "\n",
    "This notebook implements a **File Reader Agent** that can ingest media invoices and billing data in multiple formats and extract structured information for processing.\n",
    "\n",
    "The agent uses CrewAI to intelligently read and parse files, making it easy to integrate with billing reconciliation workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ CAPABILITIES\n",
    "\n",
    "**File Format Support:**\n",
    "- ðŸ“„ **PDF** - Extract text from invoice PDFs\n",
    "- ðŸ“Š **CSV** - Parse tabular billing data\n",
    "- ðŸ“ˆ **Excel** (.xlsx, .xls) - Read spreadsheet data\n",
    "- ðŸ–¼ï¸ **Images** - OCR text extraction from scanned documents\n",
    "- ðŸ“ **Text** - Plain text file reading\n",
    "- ðŸ”– **XML** - Parse XML structured data\n",
    "\n",
    "**Agent Features:**\n",
    "- Single unified agent for all file types\n",
    "- Automatic file type detection\n",
    "- Structured data extraction\n",
    "- Error handling and validation\n",
    "- Ready for CrewAI integration\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– WHAT WE'RE BUILDING\n",
    "\n",
    "A **File Reader Agent** that:\n",
    "1. Accepts file paths as input\n",
    "2. Automatically detects file format\n",
    "3. Extracts and structures the data\n",
    "4. Returns clean, parsed content\n",
    "5. Integrates with CrewAI workflows\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ USE CASE\n",
    "\n",
    "This agent serves as the **data ingestion layer** for the media billing reconciliation system:\n",
    "- **Input**: Invoice files (PDF, CSV, Excel, images, XML)\n",
    "- **Process**: Read, parse, and structure data\n",
    "- **Output**: Standardized data for reconciliation agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e36c5",
   "metadata": {},
   "source": [
    "## âœ¨ Enhanced Excel Reading Capabilities\n",
    "\n",
    "The Excel file reader has been significantly improved with the following features:\n",
    "\n",
    "### ðŸŽ¯ What's New:\n",
    "\n",
    "1. **Multi-Sheet Support**\n",
    "   - Automatically detects all sheets in Excel files\n",
    "   - Reads all sheets or specific sheets by name/index\n",
    "   - Provides sheet-by-sheet analysis\n",
    "\n",
    "2. **Intelligent Data Analysis**\n",
    "   - Column data type detection (numeric, text, dates)\n",
    "   - Statistical summaries for numeric columns\n",
    "   - Missing value analysis with percentages\n",
    "   - Unique value counting for pattern detection\n",
    "\n",
    "3. **Invoice/Billing Detection**\n",
    "   - Automatically identifies invoice-related data\n",
    "   - Detects columns containing: invoice numbers, dates, amounts, vendors, etc.\n",
    "   - Flags sheets that likely contain billing information\n",
    "\n",
    "4. **Better Readability**\n",
    "   - Structured output with clear sections\n",
    "   - Metadata about file and sheet structure\n",
    "   - Preview of data with proper formatting\n",
    "   - Row and column summaries\n",
    "\n",
    "5. **Helper Functions**\n",
    "   - `analyze_excel_structure()` - Deep structural analysis\n",
    "   - `get_excel_summary()` - Quick overview with concise mode\n",
    "   - `get_excel_column_summary()` - Just column names (very lightweight)\n",
    "   - `read_excel_content()` - Full content extraction with metadata\n",
    "\n",
    "### âš¡ Optimization for Large Files\n",
    "\n",
    "**NEW**: The reader is now optimized to prevent token overflow errors:\n",
    "- Limits to **50 rows per sheet** by default (configurable)\n",
    "- Shows **first 3 sheets only** when reading all sheets\n",
    "- **Verbose mode disabled** by default (no detailed statistics)\n",
    "- **Auto-truncates** output if > 25,000 characters\n",
    "- Column width limited to 30 characters in preview\n",
    "\n",
    "**For large Excel files:**\n",
    "```python\n",
    "# Option 1: Read specific sheet only\n",
    "content = read_excel_content('large_file.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Option 2: Get just column structure (no data)\n",
    "columns = get_excel_column_summary('large_file.xlsx')\n",
    "\n",
    "# Option 3: Use concise summary\n",
    "summary = get_excel_summary('large_file.xlsx', concise=True)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "37650ed3-b2ec-43c3-8f97-42287718443d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# ============================================\n",
    "# Uncomment and run to install all dependencies\n",
    "\n",
    "# !pip install pdfplumber pandas openpyxl pillow pytesseract lxml\n",
    "# !pip install crewai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5f0ad",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for file reading and CrewAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e26bfd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "   â€¢ File readers: pdfplumber, pandas, PIL, pytesseract, xml\n",
      "   â€¢ CrewAI: Agent, Task, Crew, LLM\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORT ALL REQUIRED LIBRARIES\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# File reading libraries\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"   â€¢ File readers: pdfplumber, pandas, PIL, pytesseract, xml\")\n",
    "print(\"   â€¢ CrewAI: Agent, Task, Crew, LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "849307b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Individual file reader functions created!\n",
      "   â€¢ read_pdf_content()\n",
      "   â€¢ read_csv_content()\n",
      "   â€¢ read_excel_content() - OPTIMIZED for large files!\n",
      "   â€¢ read_image_content()\n",
      "   â€¢ read_xml_content()\n",
      "   â€¢ read_text_content()\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FILE READER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def read_pdf_content(pdf_path: str) -> str:\n",
    "    \"\"\"Read and extract text from PDF files.\"\"\"\n",
    "    try:\n",
    "        full_text = \"\"\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, 1):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    full_text += f\"\\n--- Page {page_num} ---\\n{text}\\n\"\n",
    "        return full_text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_csv_content(csv_path: str) -> str:\n",
    "    \"\"\"Read and parse CSV files.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        return df.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading CSV: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_excel_content(excel_path: str, sheet_name=None, max_rows=50, verbose=False) -> str:\n",
    "    \"\"\"\n",
    "    Read and parse Excel files with enhanced capabilities.\n",
    "    \n",
    "    Features:\n",
    "    - Auto-detects and lists all sheets\n",
    "    - Reads multiple sheets or a specific sheet\n",
    "    - Provides data summary and structure\n",
    "    - Optimized for LLM consumption (prevents token overflow)\n",
    "    \n",
    "    Args:\n",
    "        excel_path: Path to the Excel file\n",
    "        sheet_name: Specific sheet name/index to read, or None for all sheets\n",
    "        max_rows: Maximum rows to display per sheet (default: 50, reduced for LLM)\n",
    "        verbose: If True, include detailed statistics (use False for large files)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with Excel content and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load Excel file to get sheet information\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        \n",
    "        output = []\n",
    "        output.append(f\"ðŸ“Š EXCEL FILE: {Path(excel_path).name}\")\n",
    "        output.append(f\"Total Sheets: {len(sheet_names)} - {', '.join(sheet_names)}\")\n",
    "        output.append(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Determine which sheets to read\n",
    "        sheets_to_read = []\n",
    "        if sheet_name is not None:\n",
    "            sheets_to_read = [sheet_name]\n",
    "        else:\n",
    "            # Read all sheets (limit to first 3 for large files to prevent token overflow)\n",
    "            sheets_to_read = sheet_names[:3]\n",
    "            if len(sheet_names) > 3:\n",
    "                output.append(f\"âš ï¸  Showing first 3 of {len(sheet_names)} sheets (to avoid data overflow)\\n\")\n",
    "        \n",
    "        # Read and analyze each sheet\n",
    "        for idx, sheet in enumerate(sheets_to_read, 1):\n",
    "            try:\n",
    "                df = pd.read_excel(excel_path, sheet_name=sheet)\n",
    "                \n",
    "                output.append(f\"\\nðŸ“„ SHEET {idx}: '{sheet}'\")\n",
    "                output.append(f\"{'-'*70}\")\n",
    "                output.append(f\"Size: {len(df)} rows Ã— {len(df.columns)} columns\")\n",
    "                \n",
    "                # Column info - summarized\n",
    "                output.append(f\"\\nColumns ({len(df.columns)}):\")\n",
    "                for col in df.columns:\n",
    "                    dtype = df[col].dtype\n",
    "                    non_null = df[col].notna().sum()\n",
    "                    unique = df[col].nunique()\n",
    "                    output.append(f\"  â€¢ {col}: {dtype} ({non_null} non-null, {unique} unique)\")\n",
    "                \n",
    "                # Determine how many rows to show\n",
    "                rows_to_show = min(max_rows, len(df))\n",
    "                \n",
    "                # Data preview - limited rows\n",
    "                output.append(f\"\\nData Preview (showing {rows_to_show} of {len(df)} rows):\")\n",
    "                output.append(df.head(rows_to_show).to_string(index=True, max_colwidth=30))\n",
    "                \n",
    "                # Only include detailed stats if verbose=True\n",
    "                if verbose:\n",
    "                    # Statistical summary for numeric columns\n",
    "                    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "                    if len(numeric_cols) > 0:\n",
    "                        output.append(f\"\\nNumeric Summary:\")\n",
    "                        output.append(df[numeric_cols].describe().to_string())\n",
    "                    \n",
    "                    # Missing data\n",
    "                    missing_data = df.isnull().sum()\n",
    "                    if missing_data.any():\n",
    "                        output.append(f\"\\nMissing Values:\")\n",
    "                        for col, count in missing_data[missing_data > 0].items():\n",
    "                            pct = (count / len(df)) * 100\n",
    "                            output.append(f\"  â€¢ {col}: {count} ({pct:.1f}%)\")\n",
    "                else:\n",
    "                    # Just show totals for missing data\n",
    "                    total_missing = df.isnull().sum().sum()\n",
    "                    if total_missing > 0:\n",
    "                        output.append(f\"\\nTotal missing values: {total_missing}\")\n",
    "                \n",
    "                output.append(f\"\\n{'-'*70}\")\n",
    "                \n",
    "            except Exception as sheet_error:\n",
    "                output.append(f\"\\nâš ï¸  Error reading sheet '{sheet}': {str(sheet_error)}\\n\")\n",
    "        \n",
    "        result = \"\\n\".join(output)\n",
    "        \n",
    "        # Warn if output is very large\n",
    "        if len(result) > 25000:\n",
    "            return f\"âš ï¸ Excel file is very large ({len(result)} chars). Consider using sheet_name parameter.\\n\\n\" + result[:25000] + f\"\\n\\n... [Output truncated, use verbose=False or specify sheet_name]\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error reading Excel: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_image_content(image_path: str) -> str:\n",
    "    \"\"\"Extract text from images using OCR.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading image: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_xml_content(xml_path: str) -> str:\n",
    "    \"\"\"Parse and read XML files.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        return ET.tostring(root, encoding='unicode')\n",
    "    except Exception as e:\n",
    "        return f\"Error reading XML: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_text_content(text_path: str) -> str:\n",
    "    \"\"\"Read plain text files.\"\"\"\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading text file: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Individual file reader functions created!\")\n",
    "print(\"   â€¢ read_pdf_content()\")\n",
    "print(\"   â€¢ read_csv_content()\")\n",
    "print(\"   â€¢ read_excel_content() - OPTIMIZED for large files!\")\n",
    "print(\"   â€¢ read_image_content()\")\n",
    "print(\"   â€¢ read_xml_content()\")\n",
    "print(\"   â€¢ read_text_content()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b13ed92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel analysis helper functions created!\n",
      "   â€¢ analyze_excel_structure() - Deep Excel analysis\n",
      "   â€¢ get_excel_summary() - Quick summary (with concise mode)\n",
      "   â€¢ get_excel_column_summary() - Just column names (very lightweight)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXCEL-SPECIFIC HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def analyze_excel_structure(excel_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Deep analysis of Excel file structure.\n",
    "    Returns detailed information about sheets, data patterns, and potential invoice data.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results including:\n",
    "        - Sheet information\n",
    "        - Detected tables and data regions\n",
    "        - Potential header rows\n",
    "        - Column relationships\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        analysis = {\n",
    "            'file_name': Path(excel_path).name,\n",
    "            'total_sheets': len(excel_file.sheet_names),\n",
    "            'sheet_names': excel_file.sheet_names,\n",
    "            'sheets_analysis': {}\n",
    "        }\n",
    "        \n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "            \n",
    "            # Analyze this sheet\n",
    "            sheet_info = {\n",
    "                'rows': len(df),\n",
    "                'columns': len(df.columns),\n",
    "                'column_names': list(df.columns),\n",
    "                'has_unnamed_columns': any('Unnamed' in str(col) for col in df.columns),\n",
    "                'numeric_columns': list(df.select_dtypes(include=['number']).columns),\n",
    "                'text_columns': list(df.select_dtypes(include=['object']).columns),\n",
    "                'date_columns': list(df.select_dtypes(include=['datetime']).columns),\n",
    "                'empty_rows': df.isnull().all(axis=1).sum(),\n",
    "                'duplicate_rows': df.duplicated().sum(),\n",
    "            }\n",
    "            \n",
    "            # Try to detect if this looks like invoice/billing data\n",
    "            invoice_indicators = []\n",
    "            keywords = ['invoice', 'bill', 'amount', 'total', 'price', 'quantity', 'date', 'vendor', 'payment']\n",
    "            \n",
    "            for col in df.columns:\n",
    "                col_str = str(col).lower()\n",
    "                for keyword in keywords:\n",
    "                    if keyword in col_str:\n",
    "                        invoice_indicators.append(f\"Column '{col}' contains '{keyword}'\")\n",
    "            \n",
    "            # Check for data in cells\n",
    "            for col in df.columns:\n",
    "                sample_values = df[col].dropna().astype(str).head(5).tolist()\n",
    "                for keyword in keywords:\n",
    "                    if any(keyword in str(val).lower() for val in sample_values):\n",
    "                        invoice_indicators.append(f\"Column '{col}' has values matching '{keyword}'\")\n",
    "                        break\n",
    "            \n",
    "            sheet_info['invoice_indicators'] = invoice_indicators\n",
    "            sheet_info['likely_invoice_data'] = len(invoice_indicators) > 0\n",
    "            \n",
    "            analysis['sheets_analysis'][sheet_name] = sheet_info\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "def get_excel_summary(excel_path: str, concise=True) -> str:\n",
    "    \"\"\"\n",
    "    Get a summary of Excel file suitable for LLM understanding.\n",
    "    \n",
    "    Args:\n",
    "        excel_path: Path to Excel file\n",
    "        concise: If True, returns brief summary. If False, more detailed.\n",
    "    \"\"\"\n",
    "    analysis = analyze_excel_structure(excel_path)\n",
    "    \n",
    "    if 'error' in analysis:\n",
    "        return f\"Error analyzing Excel file: {analysis['error']}\"\n",
    "    \n",
    "    summary = []\n",
    "    summary.append(f\"ðŸ“Š Excel File: {analysis['file_name']}\")\n",
    "    summary.append(f\"Total Sheets: {analysis['total_sheets']}\")\n",
    "    \n",
    "    if concise:\n",
    "        # Brief summary\n",
    "        summary.append(f\"Sheets: {', '.join(analysis['sheet_names'])}\")\n",
    "        total_rows = sum(info['rows'] for info in analysis['sheets_analysis'].values())\n",
    "        summary.append(f\"Total Data Rows: {total_rows}\")\n",
    "    else:\n",
    "        # Detailed summary\n",
    "        summary.append(\"\")\n",
    "        for sheet_name, info in analysis['sheets_analysis'].items():\n",
    "            summary.append(f\"Sheet: {sheet_name}\")\n",
    "            summary.append(f\"  Size: {info['rows']} rows Ã— {info['columns']} columns\")\n",
    "            summary.append(f\"  Columns: {', '.join([str(c) for c in info['column_names'][:10]])}\")\n",
    "            \n",
    "            if info['likely_invoice_data']:\n",
    "                summary.append(f\"  âœ… Likely contains billing/invoice data\")\n",
    "                summary.append(f\"  Indicators: {'; '.join(info['invoice_indicators'][:3])}\")\n",
    "            \n",
    "            summary.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def get_excel_column_summary(excel_path: str, sheet_name=None) -> str:\n",
    "    \"\"\"\n",
    "    Get just the column names and types from an Excel file.\n",
    "    Useful for understanding structure without loading all data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        sheets = [sheet_name] if sheet_name else excel_file.sheet_names[:3]\n",
    "        \n",
    "        summary = []\n",
    "        summary.append(f\"ðŸ“‹ Column Structure: {Path(excel_path).name}\")\n",
    "        \n",
    "        for sheet in sheets:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet, nrows=0)  # Just headers\n",
    "            summary.append(f\"\\nSheet: {sheet}\")\n",
    "            summary.append(f\"Columns: {', '.join([str(c) for c in df.columns])}\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Excel analysis helper functions created!\")\n",
    "print(\"   â€¢ analyze_excel_structure() - Deep Excel analysis\")\n",
    "print(\"   â€¢ get_excel_summary() - Quick summary (with concise mode)\")\n",
    "print(\"   â€¢ get_excel_column_summary() - Just column names (very lightweight)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a23184b0-3a10-4523-982d-7df0b66593da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CrewAI File Reader Tool created!\n",
      "   Tool name: 'read_file_tool'\n",
      "   Supports: PDF, CSV, Excel (Optimized!), Images, XML, Text\n",
      "   Excel Features: Auto-limited to 50 rows/sheet to prevent token overflow\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIVERSAL FILE READER TOOL (CrewAI Tool)\n",
    "# ============================================\n",
    "\n",
    "@tool(\"read_file_tool\")\n",
    "def read_file_tool(file_path: str, sheet_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Universal file reader tool that automatically detects and reads various file formats.\n",
    "    Supports: PDF, CSV, Excel (with multi-sheet support), Images (OCR), XML, and Text files.\n",
    "    \n",
    "    OPTIMIZED for AI agents - prevents token overflow on large Excel files.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        sheet_name: (Optional) For Excel files, specify sheet name or index to read.\n",
    "                   If None, reads first 3 sheets. Examples: \"Sheet1\", 0, \"Invoice Data\"\n",
    "        \n",
    "    Returns:\n",
    "        Extracted content as string with detailed formatting and metadata\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return f\"âŒ Error: File not found at {file_path}\"\n",
    "    \n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    print(f\"ðŸ“‚ Reading file: {file_path.name} (type: {suffix})\")\n",
    "    \n",
    "    # Route to appropriate reader based on file extension\n",
    "    if suffix == '.pdf':\n",
    "        content = read_pdf_content(str(file_path))\n",
    "    elif suffix == '.csv':\n",
    "        content = read_csv_content(str(file_path))\n",
    "    elif suffix in ['.xlsx', '.xls']:\n",
    "        # Use enhanced Excel reader with optimization for LLM\n",
    "        print(f\"   Using optimized Excel reader (max 50 rows/sheet)...\")\n",
    "        if sheet_name:\n",
    "            print(f\"   Reading specific sheet: {sheet_name}\")\n",
    "        # Use verbose=False to prevent token overflow\n",
    "        content = read_excel_content(str(file_path), sheet_name=sheet_name, max_rows=50, verbose=False)\n",
    "    elif suffix in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "        content = read_image_content(str(file_path))\n",
    "    elif suffix == '.xml':\n",
    "        content = read_xml_content(str(file_path))\n",
    "    elif suffix == '.txt':\n",
    "        content = read_text_content(str(file_path))\n",
    "    else:\n",
    "        content = f\"âš ï¸ Unsupported file type: {suffix}\"\n",
    "    \n",
    "    print(f\"âœ… Successfully read {len(content)} characters\")\n",
    "    return content\n",
    "\n",
    "print(\"âœ… CrewAI File Reader Tool created!\")\n",
    "print(\"   Tool name: 'read_file_tool'\")\n",
    "print(\"   Supports: PDF, CSV, Excel (Optimized!), Images, XML, Text\")\n",
    "print(\"   Excel Features: Auto-limited to 50 rows/sheet to prevent token overflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4a6063a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Excel metadata tool created!\n",
      "   Tool name: 'get_excel_info'\n",
      "   Purpose: Lightweight Excel file inspection (minimal tokens)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ADDITIONAL HELPER TOOL FOR EXCEL METADATA\n",
    "# ============================================\n",
    "\n",
    "@tool(\"get_excel_info\")\n",
    "def get_excel_info(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Get lightweight metadata about an Excel file without loading all data.\n",
    "    Use this FIRST before reading full Excel content to understand structure.\n",
    "    \n",
    "    Returns:\n",
    "    - File name\n",
    "    - Total sheets and their names\n",
    "    - Total rows across all sheets\n",
    "    - Brief indication if likely contains invoice/billing data\n",
    "    \n",
    "    This is very fast and uses minimal tokens.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return f\"âŒ Error: File not found at {file_path}\"\n",
    "    \n",
    "    if file_path.suffix.lower() not in ['.xlsx', '.xls']:\n",
    "        return f\"âš ï¸ Not an Excel file: {file_path.suffix}\"\n",
    "    \n",
    "    return get_excel_summary(str(file_path), concise=True)\n",
    "\n",
    "print(\"âœ… Excel metadata tool created!\")\n",
    "print(\"   Tool name: 'get_excel_info'\")\n",
    "print(\"   Purpose: Lightweight Excel file inspection (minimal tokens)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a19f8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key loaded\n",
      "âœ… LLM initialized: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD ENVIRONMENT AND INITIALIZE LLM\n",
    "# ============================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"âš ï¸  Warning: OPENAI_API_KEY not found in environment\")\n",
    "    print(\"   Please add it to your .env file\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key loaded\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM initialized: gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cd34681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File Reader Agent created!\n",
      "   Role: File Data Extraction Specialist\n",
      "   Tools: ['get_excel_info', 'read_file_tool']\n",
      "   âš¡ Optimized for large Excel files!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE FILE READER AGENT\n",
    "# ============================================\n",
    "\n",
    "file_reader_agent = Agent(\n",
    "    role='File Data Extraction Specialist',\n",
    "    goal='Read and extract data from various file formats including PDF, CSV, Excel, images, XML, and text files',\n",
    "    backstory=\"\"\"You are an expert in data extraction and file parsing. \n",
    "    You can read any type of document - from PDFs and spreadsheets to scanned images \n",
    "    and XML files. You extract information accurately and structure it in a clear, \n",
    "    usable format. You handle invoices, billing documents, and financial records \n",
    "    with precision.\n",
    "    \n",
    "    IMPORTANT: For Excel files, always use get_excel_info FIRST to understand \n",
    "    the file structure before reading full content. This prevents token overflow.\"\"\",\n",
    "    llm=llm,\n",
    "    tools=[get_excel_info, read_file_tool],\n",
    "    verbose=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "print(\"âœ… File Reader Agent created!\")\n",
    "print(f\"   Role: {file_reader_agent.role}\")\n",
    "print(f\"   Tools: {[tool.name for tool in file_reader_agent.tools]}\")\n",
    "print(\"   âš¡ Optimized for large Excel files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d374e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File reading task creator function defined!\n",
      "   Usage: task = create_file_reading_task('path/to/file.pdf')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE FILE READING TASK\n",
    "# ============================================\n",
    "\n",
    "def create_file_reading_task(file_path: str, extraction_focus: str = \"all data\") -> Task:\n",
    "    \"\"\"\n",
    "    Create a task for reading and extracting data from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        extraction_focus: What specific data to focus on (default: \"all data\")\n",
    "    \n",
    "    Returns:\n",
    "        Task object configured for file reading\n",
    "    \"\"\"\n",
    "    task = Task(\n",
    "        description=f\"\"\"Read and extract data from the following file:\n",
    "        \n",
    "File Path: {file_path}\n",
    "Extraction Focus: {extraction_focus}\n",
    "\n",
    "Your tasks:\n",
    "1. Use the read_file_tool to read the file\n",
    "2. Extract and structure the relevant information\n",
    "3. If it's an invoice or billing document, identify:\n",
    "   - Vendor/supplier name\n",
    "   - Invoice number and date\n",
    "   - Line items with descriptions and amounts\n",
    "   - Total amount\n",
    "   - Any other relevant billing information\n",
    "4. Present the extracted data in a clear, structured format\n",
    "\n",
    "Provide a comprehensive summary of the file contents.\"\"\",\n",
    "        agent=file_reader_agent,\n",
    "        expected_output=\"\"\"A structured summary of the file contents including:\n",
    "        - File type and basic metadata\n",
    "        - Key data points extracted\n",
    "        - For invoices: vendor, invoice number, date, line items, total\n",
    "        - Any notable information or patterns found\"\"\"\n",
    "    )\n",
    "    \n",
    "    return task\n",
    "\n",
    "print(\"âœ… File reading task creator function defined!\")\n",
    "print(\"   Usage: task = create_file_reading_task('path/to/file.pdf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062170f",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 6: Create and Run Crew\n",
    "\n",
    "Assemble the crew and execute the file reading task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "952f3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File processing function created!\n",
      "   Usage: result = process_file('path/to/invoice.pdf')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE AND RUN FILE READER CREW\n",
    "# ============================================\n",
    "\n",
    "def process_file(file_path: str, extraction_focus: str = \"all data\"):\n",
    "    \"\"\"\n",
    "    Process a file using the File Reader Agent and Crew.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to process\n",
    "        extraction_focus: Specific data to focus on\n",
    "        \n",
    "    Returns:\n",
    "        Extracted data and analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ Processing File: {Path(file_path).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create task for this file\n",
    "    task = create_file_reading_task(file_path, extraction_focus)\n",
    "    \n",
    "    # Create crew\n",
    "    crew = Crew(\n",
    "        agents=[file_reader_agent],\n",
    "        tasks=[task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Execute\n",
    "    result = crew.kickoff()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… File Processing Complete!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… File processing function created!\")\n",
    "print(\"   Usage: result = process_file('path/to/invoice.pdf')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
