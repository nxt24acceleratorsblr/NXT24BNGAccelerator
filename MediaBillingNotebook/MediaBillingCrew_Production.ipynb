{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26481ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS AND DEPENDENCIES\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# File reading libraries\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not found in environment\")\n",
    "    print(\"   Please add it to your .env file\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key loaded\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.1  # Low temperature for consistent extraction\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized: gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9675dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CANONICAL INVOICE SCHEMA DEFINITION\n",
    "# ============================================\n",
    "CANONICAL_SCHEMA_DOC = \"\"\"\n",
    "You MUST output JSON ARRAY matching this structure:\n",
    " \n",
    "{\n",
    "  \"invoice_header\": {\n",
    "    \"invoice_number\": string or null,\n",
    "    \"vendor_name\": string or null,\n",
    "    \"invoice_date\": string or null,\n",
    "    \"billing_start_date\": string or null,\n",
    "    \"billing_end_date\": string or null,\n",
    "    \"currency\": string or null,\n",
    "    \"gross_revenue\": number or null,\n",
    "    \"discount_amount\": number or null,\n",
    "    \"discount_percent\": number or null,\n",
    "    \"tax\": number or null\n",
    "  },\n",
    "  \"line_items\": [\n",
    "    {\n",
    "      \"line_id\": 1,\n",
    "      \"campaign_name\": \"Example Campaign\",\n",
    "      \"campaign_id\": \"12345\",\n",
    "      \"insertion_order_ID\": \"IO-900\",\n",
    "      \"start_date\": \"2025-10-14\",\n",
    "      \"end_date\": \"2025-10-30\",\n",
    "      \"duration_days\": 17,\n",
    "      \"booked_impressions\": 400000,\n",
    "      \"billed_impressions\": 350000,\n",
    "      \"views\": 5000,\n",
    "      \"gross_revenue\": 3500,\n",
    "      \"net_revenue\": 3000,\n",
    "      \"discount_amount\": 500,\n",
    "      \"discount_percent\": 5,\n",
    "      \"profit\": 1000,\n",
    "      \"rate_type\": \"CPM\",\n",
    "      \"rate\": 5.0\n",
    "    }\n",
    "  ]\n",
    "}\n",
    " \n",
    "RULES:\n",
    "- If a value is not present in the invoice, use null.\n",
    "- MANDATORY: Always extract start_date, end_date from the Dates column and calculate duration_days\n",
    "- For duration_days: If Dates column shows \"2025-10-14 to 2025-10-30\", then start_date=\"2025-10-14\", end_date=\"2025-10-30\", duration_days=17 (count includes both dates: Oct 14,15,16...30 = 17 days)\n",
    "- If only one type of revenue is present, store it in gross_revenue and leave net_revenue null (or vice versa if clearly net).\n",
    "- Discounts can be explicit (discount column) or implicit (difference between gross and net) ‚Äî explain in notes if inferred.\n",
    "- Profit = revenue - cost, if not directly provided.\n",
    "- Be conservative: do NOT invent numbers if they are not in the invoice.\n",
    "- Campaign ID can be the segment ID\n",
    "- Insertion order id can not be same as Campaign ID. Insertion order id can be a short form\n",
    "- IMPORTANT: For duration_days, parse the Dates column (format: \"YYYY-MM-DD to YYYY-MM-DD\"), extract start and end dates, then calculate the number of days INCLUDING both start and end dates (end_date - start_date + 1)\n",
    "\"\"\"\n",
    "print(\"‚úÖ Canonical schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FILE READING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def read_pdf_content(pdf_path: str, max_pages: int = 5) -> str:\n",
    "    \"\"\"Extract text from PDF files with OCR fallback for image-based PDFs.\"\"\"\n",
    "    try:\n",
    "        pages_text = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                if i >= max_pages:\n",
    "                    break\n",
    "                text = page.extract_text() or \"\"\n",
    "                \n",
    "                # If no text extracted, try OCR on the page image\n",
    "                if not text.strip():\n",
    "                    try:\n",
    "                        # Convert page to image and use OCR\n",
    "                        page_image = page.to_image(resolution=300).original\n",
    "                        text = pytesseract.image_to_string(page_image)\n",
    "                    except pytesseract.TesseractNotFoundError:\n",
    "                        text = \"\"\"[ERROR: Tesseract OCR not installed]\n",
    "                        \n",
    "To install Tesseract:\n",
    "‚Ä¢ macOS: brew install tesseract\n",
    "‚Ä¢ Ubuntu/Debian: sudo apt-get install tesseract-ocr\n",
    "‚Ä¢ Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\"\"\"\n",
    "                    except Exception as ocr_error:\n",
    "                        text = f\"[OCR failed for page {i+1}: {str(ocr_error)}]\"\n",
    "                \n",
    "                if text:\n",
    "                    pages_text.append(f\"--- Page {i+1} ---\\n{text}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(pages_text) if pages_text else \"No text extracted from PDF\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_image_content(image_path: str) -> str:\n",
    "    \"\"\"Extract text from images using OCR.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip() if text.strip() else \"No text extracted from image\"\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        return \"\"\"ERROR: Tesseract OCR is not installed.\n",
    "        \n",
    "To install Tesseract:\n",
    "‚Ä¢ macOS: brew install tesseract\n",
    "‚Ä¢ Ubuntu/Debian: sudo apt-get install tesseract-ocr\n",
    "‚Ä¢ Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\n",
    "\n",
    "After installation, restart your kernel.\"\"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading image: {str(e)}\"\n",
    "\n",
    "\n",
    "def read_excel_content(excel_path: str, sheet_name=None, max_rows=50) -> Dict[str, Any]:\n",
    "    \"\"\"Read Excel file and return structured preview.\"\"\"\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(excel_path)\n",
    "        \n",
    "        # Determine which sheet to read\n",
    "        if sheet_name is not None:\n",
    "            sheets = [sheet_name]\n",
    "        else:\n",
    "            # Read first sheet only for production\n",
    "            sheets = [excel_file.sheet_names[0]]\n",
    "        \n",
    "        result = {\n",
    "            \"file_name\": Path(excel_path).name,\n",
    "            \"total_sheets\": len(excel_file.sheet_names),\n",
    "            \"sheet_names\": excel_file.sheet_names,\n",
    "            \"data\": {}\n",
    "        }\n",
    "        \n",
    "        for sheet in sheets:\n",
    "            df = pd.read_excel(excel_path, sheet_name=sheet)\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = [\n",
    "                str(c).strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "                for c in df.columns\n",
    "            ]\n",
    "            \n",
    "            # Limit rows\n",
    "            preview_df = df.head(max_rows)\n",
    "            \n",
    "            result[\"data\"][sheet] = {\n",
    "                \"total_rows\": len(df),\n",
    "                \"columns\": list(df.columns),\n",
    "                \"preview\": preview_df.to_dict(orient=\"records\"),\n",
    "                \"preview_text\": preview_df.to_string(index=False, max_colwidth=30)\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def read_csv_content(csv_path: str, max_rows=50) -> Dict[str, Any]:\n",
    "    \"\"\"Read CSV file and return structured preview.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = [\n",
    "            str(c).strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            for c in df.columns\n",
    "        ]\n",
    "        \n",
    "        preview_df = df.head(max_rows)\n",
    "        \n",
    "        return {\n",
    "            \"file_name\": Path(csv_path).name,\n",
    "            \"total_rows\": len(df),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"preview\": preview_df.to_dict(orient=\"records\"),\n",
    "            \"preview_text\": preview_df.to_string(index=False, max_colwidth=30)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def read_text_content(text_path: str) -> str:\n",
    "    \"\"\"Read plain text files.\"\"\"\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading text file: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ File reading functions created (with OCR support for images and image-based PDFs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INVOICE CONTEXT BUILDER\n",
    "# ============================================\n",
    "\n",
    "def build_invoice_context(file_path: str, max_rows: int = 50) -> str:\n",
    "    \"\"\"\n",
    "    Build formatted context from any invoice file type.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to invoice file\n",
    "        max_rows: Maximum rows for tabular data\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with invoice content\n",
    "    \"\"\"\n",
    "    file_path_obj = Path(file_path)\n",
    "    \n",
    "    if not file_path_obj.exists():\n",
    "        return f\"ERROR: File not found: {file_path}\"\n",
    "    \n",
    "    suffix = file_path_obj.suffix.lower()\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"FILE: {file_path_obj.name}\")\n",
    "    output.append(\"=\" * 70)\n",
    "    \n",
    "    # PDF files\n",
    "    if suffix == '.pdf':\n",
    "        output.append(\"TYPE: PDF Invoice\")\n",
    "        output.append(\"\\nCONTENT:\")\n",
    "        content = read_pdf_content(str(file_path))\n",
    "        output.append(content)\n",
    "    \n",
    "    # Excel files\n",
    "    elif suffix in ['.xlsx', '.xls']:\n",
    "        output.append(\"TYPE: Excel Spreadsheet\")\n",
    "        data = read_excel_content(str(file_path), max_rows=max_rows)\n",
    "        \n",
    "        if \"error\" in data:\n",
    "            output.append(f\"\\nERROR: {data['error']}\")\n",
    "        else:\n",
    "            output.append(f\"\\nSheets: {', '.join(data['sheet_names'])}\")\n",
    "            for sheet_name, sheet_data in data['data'].items():\n",
    "                output.append(f\"\\n--- Sheet: {sheet_name} ---\")\n",
    "                output.append(f\"Total Rows: {sheet_data['total_rows']}\")\n",
    "                output.append(f\"Columns: {', '.join(sheet_data['columns'])}\")\n",
    "                output.append(f\"\\nData Preview (first {max_rows} rows):\")\n",
    "                output.append(sheet_data['preview_text'])\n",
    "    \n",
    "    # CSV files\n",
    "    elif suffix == '.csv':\n",
    "        output.append(\"TYPE: CSV File\")\n",
    "        data = read_csv_content(str(file_path), max_rows=max_rows)\n",
    "        \n",
    "        if \"error\" in data:\n",
    "            output.append(f\"\\nERROR: {data['error']}\")\n",
    "        else:\n",
    "            output.append(f\"\\nTotal Rows: {data['total_rows']}\")\n",
    "            output.append(f\"Columns: {', '.join(data['columns'])}\")\n",
    "            output.append(f\"\\nData Preview (first {max_rows} rows):\")\n",
    "            output.append(data['preview_text'])\n",
    "    \n",
    "    # Image files\n",
    "    elif suffix in ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']:\n",
    "        output.append(\"TYPE: Image File (OCR Extraction)\")\n",
    "        output.append(\"\\nCONTENT:\")\n",
    "        content = read_image_content(str(file_path))\n",
    "        output.append(content)\n",
    "    \n",
    "    # Text files\n",
    "    elif suffix == '.txt':\n",
    "        output.append(\"TYPE: Text Invoice\")\n",
    "        output.append(\"\\nCONTENT:\")\n",
    "        content = read_text_content(str(file_path))\n",
    "        output.append(content)\n",
    "    \n",
    "    else:\n",
    "        output.append(f\"ERROR: Unsupported file type: {suffix}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Invoice context builder created (supports PDF, Excel, CSV, Images, Text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREWAI AGENT DEFINITION\n",
    "# ============================================\n",
    "\n",
    "invoice_extraction_agent = Agent(\n",
    "    role='Media Invoice Data Extraction Specialist',\n",
    "    goal='Extract structured financial and delivery data from media invoices into canonical JSON format, including accurate campaign duration calculations',\n",
    "    backstory=\"\"\"You are an expert in media billing and invoice processing. \n",
    "    You understand advertising metrics (impressions, views, clicks), financial terms \n",
    "    (revenue, costs, discounts, profit), and how to extract data accurately from \n",
    "    various invoice formats including OCR-extracted text from images and scanned PDFs.\n",
    "    \n",
    "    You are skilled at handling noisy or imperfectly formatted text from OCR, identifying\n",
    "    patterns, and extracting meaningful data even when formatting is inconsistent.\n",
    "    You always follow the canonical schema strictly and never invent data - you use null \n",
    "    for missing values. When dealing with OCR text, you intelligently parse tables and \n",
    "    structured data even when spacing or alignment is imperfect.\n",
    "    \n",
    "    You are meticulous about extracting date ranges from the Dates column and calculating\n",
    "    campaign duration in days. You parse date ranges like \"2025-10-14 to 2025-10-30\" and\n",
    "    calculate duration_days as the number of days from start to end, inclusive (e.g., \n",
    "    Oct 14 to Oct 30 = 17 days, calculated as: (30-14)+1 = 17).\"\"\",\n",
    "    llm=llm,\n",
    "    tools=[],  # No tools needed - direct file reading\n",
    "    verbose=True,\n",
    "\n",
    "    allow_delegation=Falseprint(f\"   Role: {invoice_extraction_agent.role}\")\n",
    "\n",
    ")print(\"‚úÖ Invoice extraction agent created (optimized for OCR text handling)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TASK CREATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def create_extraction_task(file_path: str, max_rows: int = 50) -> Task:\n",
    "    \"\"\"\n",
    "    Create extraction task with invoice context and schema.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to invoice file\n",
    "        max_rows: Maximum rows for tabular data\n",
    "    \n",
    "    Returns:\n",
    "        Task configured for invoice extraction\n",
    "    \"\"\"\n",
    "    # Build context from file\n",
    "    context_str = build_invoice_context(file_path, max_rows=max_rows)\n",
    "    \n",
    "    description = f\"\"\"\n",
    "Extract structured invoice data from the provided file and map it to the canonical schema.\n",
    "\n",
    "**CANONICAL SCHEMA:**\n",
    "{CANONICAL_SCHEMA_DOC}\n",
    "\n",
    "**INVOICE DATA:**\n",
    "{context_str}\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1. Identify invoice header information (vendor, dates, totals, currency)\n",
    "2. Extract all line items with sequential line_id starting from 1\n",
    "3. Map financial metrics (revenue, costs, discounts, profit)\n",
    "4. Map delivery metrics (impressions, views, clicks)\n",
    "5. **CRITICAL: Extract date ranges from the Dates column and calculate duration_days:**\n",
    "   - Parse the Dates field (format: \"YYYY-MM-DD to YYYY-MM-DD\")\n",
    "   - Extract start_date and end_date separately\n",
    "   - Calculate duration_days = (end_date - start_date) + 1 (inclusive count)\n",
    "   - Example: \"2025-10-14 to 2025-10-30\" ‚Üí start_date: \"2025-10-14\", end_date: \"2025-10-30\", duration_days: 17\n",
    "6. Calculate implicit discounts if gross and net revenue differ\n",
    "7. Use null for missing values - DO NOT INVENT DATA\n",
    "8. For OCR-extracted text: Look for patterns and table structures even if spacing/formatting is imperfect\n",
    "9. Handle OCR artifacts gracefully (e.g., misread characters, spacing issues)\n",
    "10. Add clarifications to 'notes' field if needed or if OCR quality affected extraction\n",
    "11. Return ONLY valid JSON - no markdown, no explanations\n",
    "\n",
    "**OUTPUT REQUIREMENT:**\n",
    "Return a single valid JSON object following the canonical schema exactly.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    task = Task(\n",
    "        description=description,\n",
    "        agent=invoice_extraction_agent,\n",
    "        expected_output=\"Valid JSON object with invoice_header, line_items array, and notes field\"\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Task creation function defined (with OCR-specific instructions)\")\n",
    "\n",
    "    return task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7289a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MAIN EXTRACTION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def extract_invoice_data(file_path: str, max_rows: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured invoice data from any supported file format.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to invoice file (PDF, Excel, CSV, or text)\n",
    "        max_rows: Maximum rows to process from tabular files\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extracted invoice data in canonical format\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìÑ EXTRACTING INVOICE DATA\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"File: {Path(file_path).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create task\n",
    "    task = create_extraction_task(file_path, max_rows=max_rows)\n",
    "    \n",
    "    # Create crew with single agent\n",
    "    crew = Crew(\n",
    "        agents=[invoice_extraction_agent],\n",
    "        tasks=[task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Execute extraction\n",
    "    result = crew.kickoff()\n",
    "    result_str = str(result).strip()\n",
    "    \n",
    "    # Parse JSON from result\n",
    "    try:\n",
    "        parsed = json.loads(result_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON from response\n",
    "        start = result_str.find(\"{\")\n",
    "        end = result_str.rfind(\"}\")\n",
    "        \n",
    "        if start != -1 and end != -1 and start < end:\n",
    "            json_str = result_str[start : end + 1]\n",
    "            try:\n",
    "                parsed = json.loads(json_str)\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": \"Failed to parse JSON response\",\n",
    "                    \"details\": str(e),\n",
    "                    \"raw_response\": result_str[:500]\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                \"error\": \"No JSON object found in response\",\n",
    "                \"raw_response\": result_str[:500]\n",
    "            }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ EXTRACTION COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Post-process: Calculate duration_days if missing\n",
    "    parsed = calculate_missing_durations(parsed)\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "\n",
    "def calculate_missing_durations(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Post-process extracted data to calculate duration_days if missing.\n",
    "    Ensures all line items have duration_days calculated from start_date and end_date.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if \"line_items\" not in data or not isinstance(data[\"line_items\"], list):\n",
    "        return data\n",
    "    \n",
    "    for item in data[\"line_items\"]:\n",
    "        # Only calculate if duration_days is missing but we have dates\n",
    "        if item.get(\"duration_days\") is None:\n",
    "            start_date = item.get(\"start_date\")\n",
    "            end_date = item.get(\"end_date\")\n",
    "            \n",
    "            if start_date and end_date:\n",
    "                try:\n",
    "                    # Parse dates\n",
    "                    start = datetime.strptime(str(start_date).strip(), '%Y-%m-%d')\n",
    "                    end = datetime.strptime(str(end_date).strip(), '%Y-%m-%d')\n",
    "                    \n",
    "                    # Calculate duration (inclusive)\n",
    "                    duration = (end - start).days + 1\n",
    "                    item[\"duration_days\"] = duration if duration > 0 else None\n",
    "                    \n",
    "                    print(f\"   ‚úì Calculated duration for {item.get('campaign_name', 'Unknown')}: {duration} days\")\n",
    "                except (ValueError, AttributeError) as e:\n",
    "                    print(f\"   ‚ö† Could not calculate duration for {item.get('campaign_name', 'Unknown')}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main extraction function created\")\n",
    "print(\"   Usage: result = extract_invoice_data('invoice.xlsx')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VALIDATION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def validate_extracted_data(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate extracted data against canonical schema.\n",
    "    \n",
    "    Returns:\n",
    "        Validation report with errors and warnings\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        \"valid\": True,\n",
    "        \"errors\": [],\n",
    "        \"warnings\": []\n",
    "    }\n",
    "    \n",
    "    # Check top-level structure\n",
    "    if \"invoice_header\" not in data:\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"Missing 'invoice_header' field\")\n",
    "    \n",
    "    if \"line_items\" not in data:\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"Missing 'line_items' field\")\n",
    "    elif not isinstance(data[\"line_items\"], list):\n",
    "        validation[\"valid\"] = False\n",
    "        validation[\"errors\"].append(\"'line_items' must be an array\")\n",
    "    \n",
    "    # Check header has minimal info\n",
    "    if \"invoice_header\" in data:\n",
    "        header = data[\"invoice_header\"]\n",
    "        if not header.get(\"invoice_number\") and not header.get(\"vendor_name\"):\n",
    "            validation[\"warnings\"].append(\"Missing both invoice_number and vendor_name\")\n",
    "        if not header.get(\"currency\"):\n",
    "            validation[\"warnings\"].append(\"Currency not specified\")\n",
    "    \n",
    "    # Check line items have IDs\n",
    "    if \"line_items\" in data and isinstance(data[\"line_items\"], list):\n",
    "        for idx, item in enumerate(data[\"line_items\"]):\n",
    "            if \"line_id\" not in item:\n",
    "                validation[\"warnings\"].append(f\"Line item at index {idx} missing 'line_id'\")\n",
    "    \n",
    "    return validation\n",
    "\n",
    "\n",
    "print(\"‚úÖ Validation function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def save_to_json(data: Dict[str, Any], output_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save extracted data to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: Extracted invoice data\n",
    "        output_path: Custom output path (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"data/invoice_extract_{timestamp}.json\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def convert_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert line items to pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data: Extracted invoice data\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with line items and header info\n",
    "    \"\"\"\n",
    "    if \"line_items\" not in data or not data[\"line_items\"]:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(data[\"line_items\"])\n",
    "    \n",
    "    # Add header fields to each row\n",
    "    if \"invoice_header\" in data:\n",
    "        header = data[\"invoice_header\"]\n",
    "        for key in [\"invoice_number\", \"vendor_name\", \"invoice_date\", \"currency\"]:\n",
    "            if key in header:\n",
    "                df[key] = header[key]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Export functions created\")\n",
    "print(\"   ‚Ä¢ save_to_json() - Save to JSON file\")\n",
    "print(\"   ‚Ä¢ convert_to_dataframe() - Convert to DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c1316",
   "metadata": {},
   "source": [
    "# Phase 2: Invoice Reconciliation & Discrepancy Detection\n",
    "\n",
    "This section implements Phase 2 of the invoice processing system - comparing extracted data with internal mapping files to detect discrepancies.\n",
    "\n",
    "## Features:\n",
    "- Load and normalize mapping JSON files\n",
    "- Exact match discrepancy detection\n",
    "- Fuzzy logic comparison with configurable thresholds\n",
    "- Severity classification (CRITICAL, HIGH, MEDIUM, LOW)\n",
    "- Comprehensive reporting with CSV export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ec049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PHASE 2: MAPPING DATA LOADER\n",
    "# ============================================\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def load_mapping_files(mapping_folder: str = 'mapping') -> list:\n",
    "    \"\"\"\n",
    "    Load all JSON mapping files from the mapping folder.\n",
    "    \n",
    "    Args:\n",
    "        mapping_folder: Path to folder containing mapping JSON files\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing mapping data\n",
    "    \"\"\"\n",
    "    mapping_path = Path(mapping_folder)\n",
    "    \n",
    "    if not mapping_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: Mapping folder not found: {mapping_folder}\")\n",
    "        return []\n",
    "    \n",
    "    mapping_files = list(mapping_path.glob('*.json'))\n",
    "    \n",
    "    if not mapping_files:\n",
    "        print(f\"‚ö†Ô∏è  Warning: No JSON files found in {mapping_folder}\")\n",
    "        return []\n",
    "    \n",
    "    mappings = []\n",
    "    for file_path in mapping_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                data['_source_file'] = file_path.name\n",
    "                mappings.append(data)\n",
    "                print(f\"‚úÖ Loaded: {file_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {file_path.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Total mapping files loaded: {len(mappings)}\")\n",
    "    return mappings\n",
    "\n",
    "\n",
    "def normalize_mapping_data(mapping_data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize mapping data to match canonical schema structure.\n",
    "    \n",
    "    Args:\n",
    "        mapping_data: Raw mapping data from JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Normalized data matching canonical schema\n",
    "    \"\"\"\n",
    "    header = mapping_data.get('Header', {})\n",
    "    \n",
    "    normalized = {\n",
    "        'invoice_header': {\n",
    "            'invoice_number': (\n",
    "                header.get('Invoice ID') or \n",
    "                header.get('Bill Number') or \n",
    "                header.get('Reference No.')\n",
    "            ),\n",
    "            'vendor_name': mapping_data.get('Vendor'),\n",
    "            'invoice_date': (\n",
    "                header.get('Invoice Date') or \n",
    "                header.get('Date Issued') or \n",
    "                header.get('Date')\n",
    "            ),\n",
    "            'currency': (\n",
    "                header.get('Currency Type') or \n",
    "                header.get('Currency')\n",
    "            ),\n",
    "            'total_amount': (\n",
    "                header.get('Total Due') or \n",
    "                header.get('Grand Total') or \n",
    "                header.get('Amount')\n",
    "            ),\n",
    "        },\n",
    "        'line_items': [],\n",
    "        '_source_file': mapping_data.get('_source_file', 'unknown'),\n",
    "        '_invoice_index': mapping_data.get('InvoiceIndex')\n",
    "    }\n",
    "    \n",
    "    line_items = mapping_data.get('LineItems', [])\n",
    "    for idx, item in enumerate(line_items, 1):\n",
    "        dates_str = item.get('Dates')\n",
    "        normalized_item = {\n",
    "            'line_id': idx,\n",
    "            'campaign_name': item.get('Campaign'),\n",
    "            'insertion_order_id': item.get('IO'),\n",
    "            'ad_unit': item.get('Ad Unit'),\n",
    "            'format': item.get('Format'),\n",
    "            'booked_impressions': parse_number(item.get('Booked')),\n",
    "            'billed_impressions': parse_number(item.get('Billed')),\n",
    "            'clicks': parse_number(item.get('Clicks')),\n",
    "            'rate': item.get('Rate'),\n",
    "            'discount': item.get('Discount'),\n",
    "            'net_cost': parse_currency(item.get('Net Cost')),\n",
    "            'geo': item.get('Geo'),\n",
    "            'dates': dates_str,\n",
    "            'duration_days': parse_duration(dates_str),\n",
    "            'creative': item.get('Creative'),\n",
    "            'tracking': item.get('Tracking'),\n",
    "            'notes': item.get('Notes'),\n",
    "        }\n",
    "        normalized['line_items'].append(normalized_item)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "def parse_number(value) -> Optional[float]:\n",
    "    \"\"\"Parse string number with commas to float.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            return float(value.replace(',', ''))\n",
    "        return float(value)\n",
    "    except (ValueError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_currency(value) -> Optional[float]:\n",
    "    \"\"\"Parse currency string to float.\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            clean_value = value.replace('$', '').replace(',', '').strip()\n",
    "            return float(clean_value)\n",
    "        return float(value)\n",
    "    except (ValueError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Parse duration from date range string (e.g., '2025-10-01 to 2025-10-15').\"\"\"print(\"‚úÖ Mapping data loader functions created\")\n",
    "\n",
    "print(\"‚úÖ Mapping data loader functions created\")\n",
    "    if not dates_str:\n",
    "\n",
    "        return None\n",
    "\n",
    "    try:        return None\n",
    "\n",
    "        from datetime import datetime    except (ValueError, AttributeError):\n",
    "\n",
    "        # Split on ' to ' and handle various formats        return duration if duration > 0 else None\n",
    "\n",
    "        parts = dates_str.replace(' - ', ' to ').split(' to ')        duration = (end_date - start_date).days + 1\n",
    "\n",
    "        if len(parts) != 2:        # Calculate duration in days (inclusive)\n",
    "\n",
    "            return None        \n",
    "\n",
    "                end_date = datetime.strptime(parts[1].strip(), '%Y-%m-%d')\n",
    "\n",
    "        # Try to parse dates        start_date = datetime.strptime(parts[0].strip(), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FUZZY MATCHING & COMPARISON FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def fuzzy_string_match(str1: str, str2: str) -> float:\n",
    "    \"\"\"Calculate similarity ratio between two strings (0.0 to 1.0).\"\"\"\n",
    "    if str1 is None or str2 is None:\n",
    "        return 0.0\n",
    "    s1 = str(str1).strip().lower()\n",
    "    s2 = str(str2).strip().lower()\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "\n",
    "def fuzzy_number_match(num1: float, num2: float, tolerance_percent: float = 5.0) -> dict:\n",
    "    \"\"\"Check if two numbers are within tolerance.\"\"\"\n",
    "    if num1 is None or num2 is None:\n",
    "        return {\n",
    "            'is_match': False,\n",
    "            'difference': None,\n",
    "            'difference_percent': None,\n",
    "            'within_tolerance': False\n",
    "        }\n",
    "    \n",
    "    difference = abs(num1 - num2)\n",
    "    base_value = max(abs(num1), abs(num2))\n",
    "    \n",
    "    if base_value == 0:\n",
    "        difference_percent = 0.0 if difference == 0 else 100.0\n",
    "    else:\n",
    "        difference_percent = (difference / base_value) * 100\n",
    "    \n",
    "    within_tolerance = difference_percent <= tolerance_percent\n",
    "    \n",
    "    return {\n",
    "        'is_match': within_tolerance,\n",
    "        'difference': difference,\n",
    "        'difference_percent': round(difference_percent, 2),\n",
    "        'within_tolerance': within_tolerance\n",
    "    }\n",
    "\n",
    "\n",
    "def get_discrepancy_severity(percent_diff: float) -> str:\n",
    "    \"\"\"Determine severity based on percentage difference.\"\"\"\n",
    "    if percent_diff < 1:\n",
    "        return 'LOW'\n",
    "    elif percent_diff < 5:\n",
    "        return 'MEDIUM'\n",
    "    elif percent_diff < 10:\n",
    "        return 'HIGH'\n",
    "    else:\n",
    "        return 'CRITICAL'\n",
    "\n",
    "\n",
    "def compare_line_items_fuzzy(extracted: dict, mapping: dict,\n",
    "                             string_threshold: float = 0.8,\n",
    "                             number_tolerance: float = 5.0) -> dict:\n",
    "    \"\"\"Compare line items using fuzzy matching logic.\"\"\"\n",
    "    scores = []\n",
    "    matched_fields = []\n",
    "    discrepancies = []\n",
    "    \n",
    "    # Compare campaign name (high weight)\n",
    "    campaign_similarity = fuzzy_string_match(\n",
    "        extracted.get('campaign_name'),\n",
    "        mapping.get('campaign_name')\n",
    "    )\n",
    "    if campaign_similarity >= string_threshold:\n",
    "        matched_fields.append('campaign_name')\n",
    "        scores.append(('campaign_name', campaign_similarity, 3.0))\n",
    "    \n",
    "    # Compare insertion order ID (high weight)\n",
    "    io_similarity = fuzzy_string_match(\n",
    "        extracted.get('insertion_order_id'),\n",
    "        mapping.get('insertion_order_id')\n",
    "    )\n",
    "    if io_similarity >= 0.9:\n",
    "        matched_fields.append('insertion_order_id')\n",
    "        scores.append(('insertion_order_id', io_similarity, 3.0))\n",
    "    \n",
    "    # Compare numerical fields\n",
    "    numerical_fields = {\n",
    "        'booked_impressions': 2.0,\n",
    "        'billed_impressions': 2.5,\n",
    "        'clicks': 1.5,\n",
    "        'net_cost': 2.5,\n",
    "        'gross_revenue': 2.0,\n",
    "        'net_revenue': 2.0,\n",
    "        'duration_days': 2.0\n",
    "    }\n",
    "    \n",
    "    for field, weight in numerical_fields.items():\n",
    "        ext_value = extracted.get(field)\n",
    "        map_value = mapping.get(field)\n",
    "        \n",
    "        if ext_value is not None and map_value is not None:\n",
    "            match_result = fuzzy_number_match(ext_value, map_value, number_tolerance)\n",
    "            \n",
    "            if match_result['within_tolerance']:\n",
    "                matched_fields.append(field)\n",
    "                score = 1.0 - (match_result['difference_percent'] / 100)\n",
    "                scores.append((field, score, weight))\n",
    "            else:\n",
    "                discrepancies.append({\n",
    "                    'field': field,\n",
    "                    'extracted_value': ext_value,\n",
    "                    'mapping_value': map_value,\n",
    "                    'difference': match_result['difference'],\n",
    "                    'difference_percent': match_result['difference_percent'],\n",
    "                    'severity': get_discrepancy_severity(match_result['difference_percent'])\n",
    "                })\n",
    "    \n",
    "    # Compare text fields\n",
    "    text_fields = {'ad_unit': 1.0, 'format': 1.0, 'geo': 1.0}\n",
    "    \n",
    "    for field, weight in text_fields.items():\n",
    "        similarity = fuzzy_string_match(extracted.get(field), mapping.get(field))\n",
    "        if similarity >= string_threshold:\n",
    "            matched_fields.append(field)\n",
    "            scores.append((field, similarity, weight))\n",
    "        elif similarity > 0.5:\n",
    "            discrepancies.append({\n",
    "                'field': field,\n",
    "                'extracted_value': extracted.get(field),\n",
    "                'mapping_value': mapping.get(field),\n",
    "                'similarity': round(similarity, 2),\n",
    "                'severity': 'LOW'\n",
    "            })\n",
    "    \n",
    "    # Calculate weighted overall score\n",
    "    if scores:\n",
    "        total_weighted_score = sum(score * weight for _, score, weight in scores)\n",
    "        total_weight = sum(weight for _, _, weight in scores)\n",
    "        overall_score = total_weighted_score / total_weight\n",
    "    else:\n",
    "        overall_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'overall_score': round(overall_score, 3),\n",
    "        'matched_fields': matched_fields,\n",
    "        'discrepancies': discrepancies,\n",
    "        'field_scores': [(field, round(score, 2)) for field, score, _ in scores]\n",
    "    }\n",
    "\n",
    "\n",
    "def find_fuzzy_matches(extracted_data: dict, mapping_data: list, \n",
    "                       string_threshold: float = 0.8,\n",
    "                       number_tolerance: float = 5.0) -> dict:\n",
    "    \"\"\"Find matches using fuzzy logic for more flexible comparison.\"\"\"\n",
    "    results = {\n",
    "        'fuzzy_matches': [],\n",
    "        'potential_discrepancies': [],\n",
    "        'no_match_found': []\n",
    "    }\n",
    "    \n",
    "    extracted_items = extracted_data.get('line_items', [])\n",
    "    \n",
    "    for ext_item in extracted_items:\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for mapping in mapping_data:\n",
    "            for map_item in mapping.get('line_items', []):\n",
    "                match_result = compare_line_items_fuzzy(\n",
    "                    ext_item, \n",
    "                    map_item,\n",
    "                    string_threshold,\n",
    "                    number_tolerance\n",
    "                )\n",
    "                \n",
    "                if match_result['overall_score'] > best_score:\n",
    "                    best_score = match_result['overall_score']\n",
    "                    best_match = {\n",
    "                        'mapping_file': mapping['_source_file'],\n",
    "                        'extracted_line': ext_item.get('line_id'),\n",
    "                        'mapping_line': map_item.get('line_id'),\n",
    "                        'campaign': ext_item.get('campaign_name'),\n",
    "                        'overall_score': best_score,\n",
    "                        'match_details': match_result\n",
    "                    }\n",
    "        \n",
    "        if best_match:\n",
    "            if best_score >= 0.7:\n",
    "                results['fuzzy_matches'].append(best_match)\n",
    "                \n",
    "                discrepancies = best_match['match_details'].get('discrepancies', [])\n",
    "                if discrepancies:\n",
    "                    results['potential_discrepancies'].append({\n",
    "                        **best_match,\n",
    "                        'discrepancies': discrepancies\n",
    "                    })\n",
    "            else:\n",
    "                results['no_match_found'].append({\n",
    "                    'extracted_line': ext_item.get('line_id'),\n",
    "                    'campaign': ext_item.get('campaign_name'),\n",
    "                    'io': ext_item.get('insertion_order_id'),\n",
    "                    'best_score': best_score,\n",
    "                    'reason': 'No strong match found in mapping files'\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fuzzy matching and comparison functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REPORTING & EXPORT FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def generate_discrepancy_report(fuzzy_matches: dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate a detailed discrepancy report as a DataFrame.\"\"\"\n",
    "    report_data = []\n",
    "    \n",
    "    # Process fuzzy match discrepancies\n",
    "    for disc in fuzzy_matches.get('potential_discrepancies', []):\n",
    "        for field_disc in disc.get('discrepancies', []):\n",
    "            field_name = field_disc.get('field')\n",
    "            \n",
    "            # Use more descriptive field name for duration\n",
    "            if field_name == 'duration_days':\n",
    "                display_field = 'Incorrect Duration Days'\n",
    "            else:\n",
    "                display_field = field_name\n",
    "            \n",
    "            report_data.append({\n",
    "                'Source': 'Fuzzy Match',\n",
    "                'Mapping File': disc.get('mapping_file'),\n",
    "                'Campaign': disc.get('campaign'),\n",
    "                'Line ID': disc.get('extracted_line'),\n",
    "                'Field': display_field,\n",
    "                'Extracted Value': field_disc.get('extracted_value'),\n",
    "                'Planned Value': field_disc.get('mapping_value'),\n",
    "                'Difference': field_disc.get('difference', 'N/A'),\n",
    "                'Difference %': field_disc.get('difference_percent', 'N/A'),\n",
    "                'Severity': field_disc.get('severity', 'UNKNOWN')\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(report_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3, 'UNKNOWN': 4}\n",
    "        df['_severity_rank'] = df['Severity'].map(severity_order)\n",
    "        df = df.sort_values('_severity_rank').drop('_severity_rank', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_discrepancy_report(df: pd.DataFrame, output_path: str = None) -> str:\n",
    "    \"\"\"Save discrepancy report to CSV file.\"\"\"\n",
    "    if output_path is None:\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = f'output/discrepancy_report_{timestamp}.csv'\n",
    "    \n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def print_discrepancy_summary(fuzzy_matches: dict):\n",
    "    \"\"\"Print a formatted summary of discrepancy analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä DISCREPANCY ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüîç FUZZY MATCH ANALYSIS:\")\n",
    "    print(f\"   ‚úÖ Fuzzy Matches: {len(fuzzy_matches.get('fuzzy_matches', []))}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Potential Discrepancies: {len(fuzzy_matches.get('potential_discrepancies', []))}\")\n",
    "    print(f\"   ‚ùå No Match Found: {len(fuzzy_matches.get('no_match_found', []))}\")\n",
    "    \n",
    "    if fuzzy_matches.get('no_match_found'):\n",
    "        print(\"\\n   Unmatched Items:\")\n",
    "        for item in fuzzy_matches['no_match_found'][:5]:\n",
    "            print(f\"   ‚Ä¢ Line {item.get('extracted_line')}: {item.get('campaign')} (IO: {item.get('io')})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Reporting and export functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19732ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE RECONCILIATION WORKFLOW\n",
    "# ============================================\n",
    "\n",
    "def run_invoice_reconciliation(invoice_file_path: str, \n",
    "                               mapping_folder: str = 'mapping',\n",
    "                               string_threshold: float = 0.8,\n",
    "                               number_tolerance: float = 5.0,\n",
    "                               save_report: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Complete end-to-end invoice reconciliation workflow.\n",
    "    \n",
    "    Phase 1: Extract data from invoice\n",
    "    Phase 2: Compare with mapping files and detect discrepancies\n",
    "    \n",
    "    Args:\n",
    "        invoice_file_path: Path to invoice file to process\n",
    "        mapping_folder: Folder containing mapping JSON files\n",
    "        string_threshold: Minimum similarity for fuzzy string matching\n",
    "        number_tolerance: Acceptable percentage difference for numbers\n",
    "        save_report: Whether to save the report to file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complete results including discrepancy report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ STARTING INVOICE RECONCILIATION WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========== PHASE 1: EXTRACT INVOICE DATA ==========\n",
    "    print(\"\\nüì• PHASE 1: EXTRACTING INVOICE DATA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    extracted_data = extract_invoice_data(invoice_file_path)\n",
    "    \n",
    "    if \"error\" in extracted_data:\n",
    "        print(f\"\\n‚ùå ERROR: Failed to extract invoice data\")\n",
    "        print(f\"   {extracted_data['error']}\")\n",
    "        return {\"error\": extracted_data, \"status\": \"failed\"}\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extraction complete!\")\n",
    "    print(f\"   Invoice: {extracted_data.get('invoice_header', {}).get('invoice_number', 'N/A')}\")\n",
    "    print(f\"   Vendor: {extracted_data.get('invoice_header', {}).get('vendor_name', 'N/A')}\")\n",
    "    print(f\"   Line Items: {len(extracted_data.get('line_items', []))}\")\n",
    "    \n",
    "    # ========== PHASE 2: LOAD MAPPING DATA ==========\n",
    "    print(\"\\nüìÇ PHASE 2: LOADING MAPPING DATA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    mapping_data_raw = load_mapping_files(mapping_folder)\n",
    "    \n",
    "    if not mapping_data_raw:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: No mapping files found. Skipping reconciliation.\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"extracted_data\": extracted_data,\n",
    "            \"mapping_data\": [],\n",
    "            \"warning\": \"No mapping files available\",\n",
    "            \"discrepancy_report\": []\n",
    "        }\n",
    "    \n",
    "    mapping_data = [normalize_mapping_data(m) for m in mapping_data_raw]\n",
    "    \n",
    "    # ========== PHASE 3: FUZZY MATCH ANALYSIS ==========\n",
    "    print(\"\\nüîç PHASE 3: FUZZY MATCH ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    fuzzy_matches = find_fuzzy_matches(\n",
    "        extracted_data, \n",
    "        mapping_data,\n",
    "        string_threshold,\n",
    "        number_tolerance\n",
    "    )\n",
    "    print(f\"   Fuzzy Matches: {len(fuzzy_matches['fuzzy_matches'])}\")\n",
    "    print(f\"   Potential Discrepancies: {len(fuzzy_matches['potential_discrepancies'])}\")\n",
    "    print(f\"   Unmatched Items: {len(fuzzy_matches['no_match_found'])}\")\n",
    "    \n",
    "    # ========== PHASE 4: GENERATE REPORTS ==========\n",
    "    print(\"\\nüìä PHASE 4: GENERATING REPORTS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print_discrepancy_summary(fuzzy_matches)\n",
    "    \n",
    "    discrepancy_df = generate_discrepancy_report(fuzzy_matches)\n",
    "    \n",
    "    report_path = None\n",
    "    if save_report and not discrepancy_df.empty:\n",
    "        report_path = save_discrepancy_report(discrepancy_df)\n",
    "        print(f\"\\nüíæ Discrepancy report saved to: {report_path}\")\n",
    "    \n",
    "    # ========== RETURN COMPLETE RESULTS ==========\n",
    "    results = {\n",
    "        \"status\": \"success\",\n",
    "        \"extracted_data\": extracted_data,\n",
    "        \"mapping_files_count\": len(mapping_data),\n",
    "        \"fuzzy_matches\": fuzzy_matches,\n",
    "        \"discrepancy_report\": discrepancy_df.to_dict('records') if not discrepancy_df.empty else [],\n",
    "        \"discrepancy_report_df\": discrepancy_df,\n",
    "        \"report_path\": report_path,\n",
    "        \"summary\": {\n",
    "            \"total_line_items\": len(extracted_data.get('line_items', [])),\n",
    "            \"fuzzy_matches\": len(fuzzy_matches['fuzzy_matches']),\n",
    "            \"discrepancies\": len(fuzzy_matches['potential_discrepancies']),\n",
    "            \"unmatched\": len(fuzzy_matches['no_match_found'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ RECONCILIATION WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Complete reconciliation workflow function created\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
